{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d75b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Notebook is for experimenting with features & trying to improve boosting models\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import joblib\n",
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: forward selection: use only features that improves kappa\n",
    "def featurize_table(data_df):\n",
    "    tabular_df = data_df.copy()\n",
    "    # Namelength\n",
    "    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n",
    "    \n",
    "    # Description length\n",
    "    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    \n",
    "    # Is Mixed Breed? (Breed2 is not 0)\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. Text\n",
    "    tabular_df['word_count'] = tabular_df['Description'].str.split().str.len().fillna(0)\n",
    "    tabular_df['char_count'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    tabular_df['avg_word_len'] = tabular_df['char_count'] / (tabular_df['word_count'] + 1)\n",
    "    tabular_df['num_digits'] = tabular_df['Description'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
    "    tabular_df['all_caps_ratio'] = tabular_df['Description'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(1, len(str(x))))\n",
    "\n",
    "    # 2. Measures\n",
    "    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0,1)\n",
    "    tabular_df['photo_per_pet'] = tabular_df['PhotoAmt'] / tabular_df['Quantity']\n",
    "    tabular_df['age_per_size'] = tabular_df['Age'] / tabular_df['MaturitySize'] # Needs careful handling of 0s\n",
    "    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt'] # Total Media (Engagement proxy)\n",
    "    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1) # Number of Colors (Count non-zero color columns)\n",
    "    \n",
    "    \n",
    "    # 3. Simple Interactions\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0) & (tabular_df['Breed2'].notnull())\n",
    "    tabular_df['is_specific_color'] = (tabular_df['Color2'] != 0) # Has more than 1 color    \n",
    "    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)    # Is Free? (Fee is 0)\n",
    "    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)   # Health Issue Flag (Health > 1 implies injury or condition)\n",
    "    \n",
    "    # log transform for shit and giggles\n",
    "    tabular_df['Fee'] = np.log1p(tabular_df['Fee'])\n",
    "    tabular_df['PhotoAmt'] = np.log1p(tabular_df['PhotoAmt'])\n",
    "    \n",
    "    \n",
    "    # Encode categories\n",
    "    \"\"\"\n",
    "    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n",
    "                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n",
    "                    'Sterilized', 'Health', 'State']\n",
    "    tabular_df[cat_cols] = tabular_df[cat_cols].astype('category')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop useless features\n",
    "    features_to_drop = ['svd_desc_9', 'Color3', 'VideoAmt', 'name_length', 'sentiment_magnitude','Color3', 'Health', 'num_digits', 'Dewormed', 'sentiment_polarity']\n",
    "\n",
    "    tabular_df = tabular_df.drop(features_to_drop, axis=1, inplace=False)\n",
    "    \n",
    "\n",
    "    # Drop text and ID columns\n",
    "    tabular_df.drop(['Name', 'PetID', 'Description', \"RescuerID\"], axis=1, inplace=True)\n",
    "    return tabular_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c819022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features\n",
    "def extract_sentiment_from_json(pet_id, sentiment_dir=\"../data/train_sentiment/\"):\n",
    "    # This assumes the sentiment files follow the pattern {PetID}.json\n",
    "    filename = f\"{sentiment_dir}/{pet_id}.json\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            # Usually 'documentSentiment' holds the overall score\n",
    "            if 'documentSentiment' in data:\n",
    "                return data['documentSentiment']['score'], data['documentSentiment']['magnitude']\n",
    "    except:\n",
    "        pass\n",
    "    return 0, 0 # Default if missing\n",
    "\n",
    "\n",
    "def generate_text_features(df, svd_components=20, is_train=True, fit_on_text=None):\n",
    "    \"\"\"\n",
    "    df: The dataframe (containing 'Description' and 'PetID')\n",
    "    svd_components: Number of latent features to keep\n",
    "    is_train: Boolean, used to decide whether to fit or transform\n",
    "    fit_on_text: If is_train=False, pass the vectorizers here (tuple: tfidf, svd)\n",
    "    \"\"\"\n",
    "    df_text = df.copy()\n",
    "    \n",
    "    # 1. TF-IDF + SVD (Latent Semantic Analysis)\n",
    "    print(\"Generating TF-IDF SVD features...\")\n",
    "    descriptions = df_text['Description'].fillna(\"none\").astype(str)\n",
    "    \n",
    "    if is_train:\n",
    "        # Fit on TRAINING descriptions\n",
    "        tfidf = TfidfVectorizer(min_df=3,  max_features=1000, \n",
    "                                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                                ngram_range=(1, 3), use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                                stop_words = 'english')\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "        \n",
    "        # Fit Transform\n",
    "        tf_vecs = tfidf.fit_transform(descriptions)\n",
    "        svd_vecs = svd.fit_transform(tf_vecs)\n",
    "        \n",
    "        # Save vectorizers for inference later\n",
    "        vectorizers = (tfidf, svd)\n",
    "    else:\n",
    "        # Load from passed tuple\n",
    "        tfidf, svd = fit_on_text\n",
    "        tf_vecs = tfidf.transform(descriptions)\n",
    "        svd_vecs = svd.transform(tf_vecs)\n",
    "        vectorizers = fit_on_text\n",
    "\n",
    "    # Create Columns\n",
    "    svd_df = pd.DataFrame(svd_vecs, columns=[f'svd_desc_{i}' for i in range(svd_components)])\n",
    "    # We reset index to make sure concat aligns correctly row-by-row\n",
    "    df_text = pd.concat([df_text.reset_index(drop=True), svd_df], axis=1)\n",
    "\n",
    "    # 2. Sentiment Analysis (File-based lookup)\n",
    "    # Determine directory\n",
    "    sent_dir = \"../data/train_sentiment\" if is_train else \"../data/test_sentiment\"\n",
    "    \n",
    "    print(\"Extracting Sentiment...\")\n",
    "    # Apply row-wise (can be slow, maybe parallelize with pandarallel if needed)\n",
    "    sent_data = df_text['PetID'].apply(lambda x: extract_sentiment_from_json(x, sent_dir))\n",
    "    \n",
    "    df_text['sentiment_score'] = [x[0] for x in sent_data]\n",
    "    df_text['sentiment_magnitude'] = [x[1] for x in sent_data]\n",
    "    df_text['sentiment_polarity'] = df_text['sentiment_score'] * df_text['sentiment_magnitude']\n",
    "\n",
    "    return df_text, vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8be96e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF SVD features...\n",
      "Extracting Sentiment...\n",
      "Generating TF-IDF SVD features...\n",
      "Extracting Sentiment...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>svd_desc_0</th>\n",
       "      <th>svd_desc_1</th>\n",
       "      <th>svd_desc_2</th>\n",
       "      <th>svd_desc_3</th>\n",
       "      <th>svd_desc_4</th>\n",
       "      <th>svd_desc_5</th>\n",
       "      <th>svd_desc_6</th>\n",
       "      <th>svd_desc_7</th>\n",
       "      <th>svd_desc_8</th>\n",
       "      <th>svd_desc_10</th>\n",
       "      <th>svd_desc_11</th>\n",
       "      <th>svd_desc_12</th>\n",
       "      <th>svd_desc_13</th>\n",
       "      <th>svd_desc_14</th>\n",
       "      <th>svd_desc_15</th>\n",
       "      <th>svd_desc_16</th>\n",
       "      <th>svd_desc_17</th>\n",
       "      <th>svd_desc_18</th>\n",
       "      <th>svd_desc_19</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>description_length</th>\n",
       "      <th>is_mixed_breed</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>all_caps_ratio</th>\n",
       "      <th>fee_per_pet</th>\n",
       "      <th>photo_per_pet</th>\n",
       "      <th>age_per_size</th>\n",
       "      <th>total_media</th>\n",
       "      <th>num_colors</th>\n",
       "      <th>is_specific_color</th>\n",
       "      <th>is_free</th>\n",
       "      <th>has_health_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.244532</td>\n",
       "      <td>-0.037060</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.020883</td>\n",
       "      <td>0.126230</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.043185</td>\n",
       "      <td>0.036605</td>\n",
       "      <td>0.047601</td>\n",
       "      <td>-0.064420</td>\n",
       "      <td>0.069778</td>\n",
       "      <td>-0.011776</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>-0.023213</td>\n",
       "      <td>-0.160635</td>\n",
       "      <td>-0.080065</td>\n",
       "      <td>0.086479</td>\n",
       "      <td>-0.009234</td>\n",
       "      <td>0.052999</td>\n",
       "      <td>0.3</td>\n",
       "      <td>450.0</td>\n",
       "      <td>True</td>\n",
       "      <td>80.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.129967</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>-0.011191</td>\n",
       "      <td>0.214028</td>\n",
       "      <td>-0.024347</td>\n",
       "      <td>0.093595</td>\n",
       "      <td>-0.125752</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>-0.016720</td>\n",
       "      <td>0.200051</td>\n",
       "      <td>0.089453</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>-0.018445</td>\n",
       "      <td>-0.122734</td>\n",
       "      <td>-0.038898</td>\n",
       "      <td>-0.004980</td>\n",
       "      <td>-0.025647</td>\n",
       "      <td>-0.053705</td>\n",
       "      <td>-0.021235</td>\n",
       "      <td>0.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.923077</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.036105</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>-0.040065</td>\n",
       "      <td>0.043863</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>-0.002840</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>0.023516</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>0.035405</td>\n",
       "      <td>-0.059616</td>\n",
       "      <td>-0.016858</td>\n",
       "      <td>-0.010160</td>\n",
       "      <td>-0.060419</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>-0.009877</td>\n",
       "      <td>0.079327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.230322</td>\n",
       "      <td>0.946186</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>-0.101189</td>\n",
       "      <td>-0.003106</td>\n",
       "      <td>0.061647</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>-0.022837</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.017754</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.068430</td>\n",
       "      <td>-0.074480</td>\n",
       "      <td>-0.007891</td>\n",
       "      <td>0.010836</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.136872</td>\n",
       "      <td>0.353950</td>\n",
       "      <td>-0.019017</td>\n",
       "      <td>-0.113207</td>\n",
       "      <td>-0.015904</td>\n",
       "      <td>-0.024246</td>\n",
       "      <td>0.053143</td>\n",
       "      <td>-0.041385</td>\n",
       "      <td>0.047071</td>\n",
       "      <td>-0.043739</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.024690</td>\n",
       "      <td>-0.049469</td>\n",
       "      <td>-0.065180</td>\n",
       "      <td>-0.037186</td>\n",
       "      <td>-0.137048</td>\n",
       "      <td>-0.122686</td>\n",
       "      <td>0.068733</td>\n",
       "      <td>-0.073205</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11989</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>265</td>\n",
       "      <td>299</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.057199</td>\n",
       "      <td>0.036065</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.026868</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>-0.046638</td>\n",
       "      <td>0.061714</td>\n",
       "      <td>-0.104008</td>\n",
       "      <td>0.038616</td>\n",
       "      <td>-0.016623</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>-0.001041</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>-0.085281</td>\n",
       "      <td>-0.118394</td>\n",
       "      <td>-0.003344</td>\n",
       "      <td>0.5</td>\n",
       "      <td>340.0</td>\n",
       "      <td>True</td>\n",
       "      <td>66.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>5.074627</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11990</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.224184</td>\n",
       "      <td>-0.049292</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.110897</td>\n",
       "      <td>0.027647</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>-0.007240</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>-0.065398</td>\n",
       "      <td>0.126936</td>\n",
       "      <td>-0.033020</td>\n",
       "      <td>0.036416</td>\n",
       "      <td>0.069005</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>-0.002086</td>\n",
       "      <td>-0.028476</td>\n",
       "      <td>-0.054225</td>\n",
       "      <td>0.2</td>\n",
       "      <td>313.0</td>\n",
       "      <td>False</td>\n",
       "      <td>63.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.890625</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11991</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41401</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.161576</td>\n",
       "      <td>-0.049235</td>\n",
       "      <td>0.056162</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>-0.019945</td>\n",
       "      <td>0.108630</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.015146</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>-0.032981</td>\n",
       "      <td>-0.071715</td>\n",
       "      <td>-0.088255</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.019530</td>\n",
       "      <td>0.4</td>\n",
       "      <td>284.0</td>\n",
       "      <td>True</td>\n",
       "      <td>50.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>5.568627</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11992</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.163324</td>\n",
       "      <td>-0.050198</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.046310</td>\n",
       "      <td>0.054158</td>\n",
       "      <td>-0.027762</td>\n",
       "      <td>0.085841</td>\n",
       "      <td>-0.038258</td>\n",
       "      <td>-0.070167</td>\n",
       "      <td>0.044753</td>\n",
       "      <td>-0.022347</td>\n",
       "      <td>-0.082730</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-0.048951</td>\n",
       "      <td>-0.022561</td>\n",
       "      <td>0.039550</td>\n",
       "      <td>0.035044</td>\n",
       "      <td>0.3</td>\n",
       "      <td>219.0</td>\n",
       "      <td>True</td>\n",
       "      <td>42.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.093023</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11993</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>299</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41401</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.302058</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>-0.043483</td>\n",
       "      <td>0.064189</td>\n",
       "      <td>-0.063165</td>\n",
       "      <td>-0.030575</td>\n",
       "      <td>0.096079</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>-0.109025</td>\n",
       "      <td>0.050274</td>\n",
       "      <td>0.267790</td>\n",
       "      <td>0.100039</td>\n",
       "      <td>-0.114343</td>\n",
       "      <td>0.225899</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>-0.057573</td>\n",
       "      <td>-0.010899</td>\n",
       "      <td>0.085509</td>\n",
       "      <td>0.3</td>\n",
       "      <td>347.0</td>\n",
       "      <td>True</td>\n",
       "      <td>66.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>5.179104</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11994 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type  Age  Breed1  Breed2  Gender  Color1  Color2  MaturitySize  \\\n",
       "0         1    2     307     307       1       1       0             2   \n",
       "1         1   12     307       0       2       2       0             1   \n",
       "2         1    2     307     307       1       1       2             2   \n",
       "3         1    2     307     307       1       3       0             2   \n",
       "4         2    4     265       0       3       1       2             2   \n",
       "...     ...  ...     ...     ...     ...     ...     ...           ...   \n",
       "11989     2    8     265     299       2       1       2             2   \n",
       "11990     1    2     307       0       2       6       7             1   \n",
       "11991     1    2     307     307       1       1       0             2   \n",
       "11992     1    2     307     307       1       1       2             2   \n",
       "11993     2   30     299     266       1       3       4             2   \n",
       "\n",
       "       FurLength  Vaccinated  Sterilized  Quantity  Fee  State  PhotoAmt  \\\n",
       "0              2           2           2         3  0.0  41326  2.197225   \n",
       "1              1           1           1         1  0.0  41326  1.386294   \n",
       "2              1           2           2         1  0.0  41326  0.693147   \n",
       "3              2           1           2         1  0.0  41326  1.098612   \n",
       "4              2           2           2         4  0.0  41326  1.098612   \n",
       "...          ...         ...         ...       ...  ...    ...       ...   \n",
       "11989          2           1           2         1  0.0  41326  1.098612   \n",
       "11990          1           2           3         1  0.0  41326  0.693147   \n",
       "11991          2           1           2         1  0.0  41401  1.098612   \n",
       "11992          2           2           2         1  0.0  41326  1.098612   \n",
       "11993          1           1           1         1  0.0  41401  1.098612   \n",
       "\n",
       "       svd_desc_0  svd_desc_1  svd_desc_2  svd_desc_3  svd_desc_4  svd_desc_5  \\\n",
       "0        0.244532   -0.037060    0.008682    0.020883    0.126230    0.017125   \n",
       "1        0.129967   -0.014390   -0.011191    0.214028   -0.024347    0.093595   \n",
       "2        0.036105    0.006319    0.009052   -0.040065    0.043863    0.006899   \n",
       "3        0.230322    0.946186    0.060190    0.014545   -0.101189   -0.003106   \n",
       "4        0.136872    0.353950   -0.019017   -0.113207   -0.015904   -0.024246   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11989    0.205444   -0.049414   -0.057199    0.036065    0.039162    0.026868   \n",
       "11990    0.224184   -0.049292    0.025368    0.003476    0.110897    0.027647   \n",
       "11991    0.161576   -0.049235    0.056162    0.033361   -0.003143   -0.019945   \n",
       "11992    0.163324   -0.050198   -0.018858    0.013617    0.006744   -0.046310   \n",
       "11993    0.302058    0.023961   -0.043483    0.064189   -0.063165   -0.030575   \n",
       "\n",
       "       svd_desc_6  svd_desc_7  svd_desc_8  svd_desc_10  svd_desc_11  \\\n",
       "0        0.043185    0.036605    0.047601    -0.064420     0.069778   \n",
       "1       -0.125752    0.019212   -0.016720     0.200051     0.089453   \n",
       "2       -0.002840    0.006765    0.023516     0.026478     0.035405   \n",
       "3        0.061647    0.002062    0.002932     0.000950    -0.022837   \n",
       "4        0.053143   -0.041385    0.047071    -0.043739     0.015210   \n",
       "...           ...         ...         ...          ...          ...   \n",
       "11989    0.063012   -0.002587   -0.046638     0.061714    -0.104008   \n",
       "11990    0.018800   -0.007240    0.129459    -0.065398     0.126936   \n",
       "11991    0.108630    0.023962    0.007483    -0.011080    -0.015146   \n",
       "11992    0.054158   -0.027762    0.085841    -0.038258    -0.070167   \n",
       "11993    0.096079    0.020646   -0.109025     0.050274     0.267790   \n",
       "\n",
       "       svd_desc_12  svd_desc_13  svd_desc_14  svd_desc_15  svd_desc_16  \\\n",
       "0        -0.011776    -0.090580    -0.023213    -0.160635    -0.080065   \n",
       "1         0.019649    -0.018445    -0.122734    -0.038898    -0.004980   \n",
       "2        -0.059616    -0.016858    -0.010160    -0.060419     0.012564   \n",
       "3         0.026151     0.017754     0.032926     0.068430    -0.074480   \n",
       "4         0.024690    -0.049469    -0.065180    -0.037186    -0.137048   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11989     0.038616    -0.016623     0.024413    -0.001041     0.036241   \n",
       "11990    -0.033020     0.036416     0.069005     0.006937     0.017209   \n",
       "11991     0.019132    -0.032981    -0.071715    -0.088255     0.021091   \n",
       "11992     0.044753    -0.022347    -0.082730     0.003721    -0.048951   \n",
       "11993     0.100039    -0.114343     0.225899    -0.010223    -0.010185   \n",
       "\n",
       "       svd_desc_17  svd_desc_18  svd_desc_19  sentiment_score  \\\n",
       "0         0.086479    -0.009234     0.052999              0.3   \n",
       "1        -0.025647    -0.053705    -0.021235              0.6   \n",
       "2         0.026524    -0.009877     0.079327              0.0   \n",
       "3        -0.007891     0.010836     0.008565              0.0   \n",
       "4        -0.122686     0.068733    -0.073205              0.2   \n",
       "...            ...          ...          ...              ...   \n",
       "11989    -0.085281    -0.118394    -0.003344              0.5   \n",
       "11990    -0.002086    -0.028476    -0.054225              0.2   \n",
       "11991     0.036146     0.006831     0.019530              0.4   \n",
       "11992    -0.022561     0.039550     0.035044              0.3   \n",
       "11993    -0.057573    -0.010899     0.085509              0.3   \n",
       "\n",
       "       description_length  is_mixed_breed  word_count  char_count  \\\n",
       "0                   450.0            True        80.0       450.0   \n",
       "1                    77.0           False        12.0        77.0   \n",
       "2                    29.0            True         5.0        29.0   \n",
       "3                    12.0            True         2.0        12.0   \n",
       "4                    20.0           False         3.0        20.0   \n",
       "...                   ...             ...         ...         ...   \n",
       "11989               340.0            True        66.0       340.0   \n",
       "11990               313.0           False        63.0       313.0   \n",
       "11991               284.0            True        50.0       284.0   \n",
       "11992               219.0            True        42.0       219.0   \n",
       "11993               347.0            True        66.0       347.0   \n",
       "\n",
       "       avg_word_len  all_caps_ratio  fee_per_pet  photo_per_pet  age_per_size  \\\n",
       "0          5.555556        0.008889          0.0       2.666667           1.0   \n",
       "1          5.923077        0.038961          0.0       3.000000          12.0   \n",
       "2          4.833333        0.137931          0.0       1.000000           1.0   \n",
       "3          4.000000        0.083333          0.0       2.000000           1.0   \n",
       "4          5.000000        0.050000          0.0       0.500000           2.0   \n",
       "...             ...             ...          ...            ...           ...   \n",
       "11989      5.074627        0.014706          0.0       2.000000           4.0   \n",
       "11990      4.890625        0.015974          0.0       1.000000           2.0   \n",
       "11991      5.568627        0.021127          0.0       2.000000           1.0   \n",
       "11992      5.093023        0.027397          0.0       2.000000           1.0   \n",
       "11993      5.179104        0.025937          0.0       2.000000          15.0   \n",
       "\n",
       "       total_media  num_colors  is_specific_color  is_free  has_health_issue  \n",
       "0              8.0           1              False        1                 0  \n",
       "1              3.0           1              False        1                 0  \n",
       "2              1.0           3               True        1                 0  \n",
       "3              2.0           1              False        1                 0  \n",
       "4              2.0           2               True        1                 0  \n",
       "...            ...         ...                ...      ...               ...  \n",
       "11989          2.0           2               True        1                 0  \n",
       "11990          1.0           2               True        1                 0  \n",
       "11991          2.0           1              False        1                 0  \n",
       "11992          2.0           3               True        1                 0  \n",
       "11993          2.0           3               True        1                 0  \n",
       "\n",
       "[11994 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "full_df = pd.read_csv(\"../data/train/train.csv\") #Beware of directory\n",
    "\n",
    "# Splitting the data from train.csv\n",
    "X = full_df.drop(['AdoptionSpeed'], axis=1)\n",
    "y = full_df['AdoptionSpeed'] \n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Generate  Features\n",
    "X_train_text, vec_tuple = generate_text_features(X_train_raw, is_train=True)\n",
    "X_test_text, _ = generate_text_features(X_test_raw,is_train=False, fit_on_text=vec_tuple)\n",
    "\n",
    "\n",
    "X_train = featurize_table(X_train_text)\n",
    "X_test = featurize_table(X_test_text)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc83a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan 1: normal classification\n",
    "# Hyperparameter-tuning w/Optuna\n",
    "\n",
    "# Calculate weights inversely proportional to class frequencies\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # Config for how to predict\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 5,\n",
    "        'tree_method': 'hist', # Faster training\n",
    "        #'enable_categorical': True,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Use GPU if available\n",
    "        \n",
    "        # Tuning parameters\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000), # More trees, but early stopping handles it\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0), # % of feature used per tree\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "    }\n",
    "\n",
    "    # Split for early stopping (Optuna needs a validation set)\n",
    "    # Using specific validation set (futher split from train set)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                stratify=y_train\n",
    "                                                )\n",
    "\n",
    "    # Choose regressor if trying to use with optimied rounder\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=50)\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    kappa = cohen_kappa_score(y_val, preds, weights='quadratic')\n",
    "    return kappa\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50) # Run 50 smart trials\n",
    "\n",
    "print(f\"Best trial value: {study.best_value}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Use best params\n",
    "best_params = study.best_params\n",
    "# Add fixed params back\n",
    "best_params['objective'] = 'multi:softprob'\n",
    "best_params['num_class'] = 5\n",
    "\n",
    "xgb_optuna = xgb.XGBClassifier(**best_params)\n",
    "xgb_optuna.fit(X_train, y_train, sample_weight=sample_weights) \n",
    "pred_xgb = xgb_optuna.predict(X_test)\n",
    "#joblib.dump(xgb_optuna, 'xgb_optuna_full.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c30a5e",
   "metadata": {},
   "source": [
    "XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50c9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-17 22:18:21,859]\u001b[0m A new study created in memory with name: no-name-9f2c0ef4-6ac3-4d77-b275-c93d3592dd24\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:22,934]\u001b[0m Trial 0 finished with value: 0.25886534158220353 and parameters: {'n_estimators': 929, 'learning_rate': 0.005246736250204405, 'max_depth': 4, 'subsample': 0.8679915843887357, 'colsample_bytree': 0.798761921766126, 'min_child_weight': 8, 'reg_alpha': 3.9905182129004766, 'reg_lambda': 2.7949727953175523, 'gamma': 1.3999528598157633}. Best is trial 0 with value: 0.25886534158220353.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:23,129]\u001b[0m Trial 1 finished with value: 0.25686495079034477 and parameters: {'n_estimators': 701, 'learning_rate': 0.153595051439445, 'max_depth': 8, 'subsample': 0.6355717172485571, 'colsample_bytree': 0.8528420220529289, 'min_child_weight': 15, 'reg_alpha': 9.956977761660108, 'reg_lambda': 8.806785204364289, 'gamma': 2.9733858221516303}. Best is trial 0 with value: 0.25886534158220353.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:24,474]\u001b[0m Trial 2 finished with value: 0.27954093826450954 and parameters: {'n_estimators': 775, 'learning_rate': 0.020420711684652225, 'max_depth': 11, 'subsample': 0.7764624327872036, 'colsample_bytree': 0.9971355684263266, 'min_child_weight': 15, 'reg_alpha': 8.76637268721877, 'reg_lambda': 2.4516766878806315, 'gamma': 1.1500458307441586}. Best is trial 2 with value: 0.27954093826450954.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:26,132]\u001b[0m Trial 3 finished with value: 0.2741006524864332 and parameters: {'n_estimators': 616, 'learning_rate': 0.008534547964955431, 'max_depth': 12, 'subsample': 0.7486703823635003, 'colsample_bytree': 0.6692194607915252, 'min_child_weight': 5, 'reg_alpha': 7.396570255936993, 'reg_lambda': 0.6726347496726373, 'gamma': 2.8594458026490943}. Best is trial 2 with value: 0.27954093826450954.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:26,330]\u001b[0m Trial 4 finished with value: 0.26224890610651086 and parameters: {'n_estimators': 607, 'learning_rate': 0.17585130130880708, 'max_depth': 8, 'subsample': 0.717389828241346, 'colsample_bytree': 0.9367243385526006, 'min_child_weight': 13, 'reg_alpha': 9.420887147394106, 'reg_lambda': 2.4502452176507044, 'gamma': 2.518945300528745}. Best is trial 2 with value: 0.27954093826450954.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:28,663]\u001b[0m Trial 5 finished with value: 0.3080276481397476 and parameters: {'n_estimators': 605, 'learning_rate': 0.010482526257248933, 'max_depth': 10, 'subsample': 0.6395137900666664, 'colsample_bytree': 0.8462769552272794, 'min_child_weight': 5, 'reg_alpha': 4.753979852402681, 'reg_lambda': 3.9373570757521708, 'gamma': 0.6700309503655327}. Best is trial 5 with value: 0.3080276481397476.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:29,548]\u001b[0m Trial 6 finished with value: 0.27549670254107494 and parameters: {'n_estimators': 796, 'learning_rate': 0.01972800979120257, 'max_depth': 10, 'subsample': 0.6159206494953745, 'colsample_bytree': 0.7442246436154312, 'min_child_weight': 12, 'reg_alpha': 1.7311938209052213, 'reg_lambda': 9.794518185782717, 'gamma': 3.739762910106452}. Best is trial 5 with value: 0.3080276481397476.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:30,700]\u001b[0m Trial 7 finished with value: 0.3173234412044066 and parameters: {'n_estimators': 791, 'learning_rate': 0.027673750681263775, 'max_depth': 10, 'subsample': 0.7227430920696454, 'colsample_bytree': 0.8034171437197901, 'min_child_weight': 9, 'reg_alpha': 0.40365061027454785, 'reg_lambda': 6.38759015864891, 'gamma': 1.045889372735012}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:31,105]\u001b[0m Trial 8 finished with value: 0.28897545473766484 and parameters: {'n_estimators': 541, 'learning_rate': 0.059447876976899204, 'max_depth': 4, 'subsample': 0.7708572012449628, 'colsample_bytree': 0.8508203533180716, 'min_child_weight': 5, 'reg_alpha': 8.641977203044068, 'reg_lambda': 8.628613633897478, 'gamma': 1.9169834033311166}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:31,366]\u001b[0m Trial 9 finished with value: 0.2637989045738778 and parameters: {'n_estimators': 579, 'learning_rate': 0.12749300467737287, 'max_depth': 12, 'subsample': 0.9237713474024668, 'colsample_bytree': 0.6775159386534888, 'min_child_weight': 7, 'reg_alpha': 8.141416699438956, 'reg_lambda': 4.274106092012686, 'gamma': 2.6230748270479234}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:31,810]\u001b[0m Trial 10 finished with value: 0.2965912319636316 and parameters: {'n_estimators': 929, 'learning_rate': 0.049801024342609676, 'max_depth': 6, 'subsample': 0.9902668592332794, 'colsample_bytree': 0.6227553522910143, 'min_child_weight': 11, 'reg_alpha': 0.29397920313111486, 'reg_lambda': 6.7602520911015045, 'gamma': 0.06662414569330144}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:34,157]\u001b[0m Trial 11 finished with value: 0.30905681376397853 and parameters: {'n_estimators': 850, 'learning_rate': 0.012858023369432108, 'max_depth': 9, 'subsample': 0.678244256166676, 'colsample_bytree': 0.7832003098391328, 'min_child_weight': 3, 'reg_alpha': 4.8007662367404995, 'reg_lambda': 6.121276917534498, 'gamma': 0.007561615413058664}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:35,540]\u001b[0m Trial 12 finished with value: 0.3134805101518382 and parameters: {'n_estimators': 848, 'learning_rate': 0.020988939099319006, 'max_depth': 9, 'subsample': 0.6960178639113156, 'colsample_bytree': 0.7643059641288643, 'min_child_weight': 3, 'reg_alpha': 2.886163066869461, 'reg_lambda': 6.541063172450983, 'gamma': 0.24678497462761317}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:36,042]\u001b[0m Trial 13 finished with value: 0.27556922981339116 and parameters: {'n_estimators': 991, 'learning_rate': 0.03838859936265956, 'max_depth': 6, 'subsample': 0.8369812031504775, 'colsample_bytree': 0.7338462082844094, 'min_child_weight': 10, 'reg_alpha': 2.584581498492766, 'reg_lambda': 6.540651465764607, 'gamma': 0.8509364891792774}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:36,530]\u001b[0m Trial 14 finished with value: 0.29603540552200736 and parameters: {'n_estimators': 704, 'learning_rate': 0.02263274489476848, 'max_depth': 6, 'subsample': 0.6972694374952274, 'colsample_bytree': 0.9094035772592375, 'min_child_weight': 3, 'reg_alpha': 0.5740727716911392, 'reg_lambda': 7.68163438218599, 'gamma': 4.546053710665759}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:36,980]\u001b[0m Trial 15 finished with value: 0.29815568518499536 and parameters: {'n_estimators': 828, 'learning_rate': 0.06315970763149716, 'max_depth': 10, 'subsample': 0.8333011758881178, 'colsample_bytree': 0.7349779820942982, 'min_child_weight': 7, 'reg_alpha': 2.5893437928991228, 'reg_lambda': 5.3375684279336335, 'gamma': 1.8702679705239214}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:37,833]\u001b[0m Trial 16 finished with value: 0.27792145866914064 and parameters: {'n_estimators': 868, 'learning_rate': 0.03041016900480856, 'max_depth': 9, 'subsample': 0.7200895384172848, 'colsample_bytree': 0.7685678051320033, 'min_child_weight': 10, 'reg_alpha': 6.820645341408138, 'reg_lambda': 7.509162440028533, 'gamma': 0.3776906421909665}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:38,184]\u001b[0m Trial 17 finished with value: 0.296204673926716 and parameters: {'n_estimators': 712, 'learning_rate': 0.09225265545315135, 'max_depth': 7, 'subsample': 0.6791584899411978, 'colsample_bytree': 0.6944770037736792, 'min_child_weight': 8, 'reg_alpha': 1.5500641672420579, 'reg_lambda': 5.513427495583021, 'gamma': 1.576170593540193}. Best is trial 7 with value: 0.3173234412044066.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:40,379]\u001b[0m Trial 18 finished with value: 0.3239231657373558 and parameters: {'n_estimators': 900, 'learning_rate': 0.014765133323932157, 'max_depth': 11, 'subsample': 0.8124241410164256, 'colsample_bytree': 0.836120955256684, 'min_child_weight': 13, 'reg_alpha': 3.6596656394528524, 'reg_lambda': 4.070937279766028, 'gamma': 0.7701164651327226}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:44,186]\u001b[0m Trial 19 finished with value: 0.2929316320465615 and parameters: {'n_estimators': 916, 'learning_rate': 0.006471743338989399, 'max_depth': 11, 'subsample': 0.900378700941985, 'colsample_bytree': 0.8965805189006747, 'min_child_weight': 13, 'reg_alpha': 5.601065884137564, 'reg_lambda': 4.082007186243183, 'gamma': 0.87615225380979}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:45,699]\u001b[0m Trial 20 finished with value: 0.31735912879106754 and parameters: {'n_estimators': 969, 'learning_rate': 0.014122676064509329, 'max_depth': 11, 'subsample': 0.8106000772747015, 'colsample_bytree': 0.828570823204956, 'min_child_weight': 13, 'reg_alpha': 4.02232060729209, 'reg_lambda': 1.6563700455566108, 'gamma': 2.076391382407308}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:47,295]\u001b[0m Trial 21 finished with value: 0.2751527007481557 and parameters: {'n_estimators': 995, 'learning_rate': 0.013145910527544746, 'max_depth': 11, 'subsample': 0.811466754898818, 'colsample_bytree': 0.8279792901920449, 'min_child_weight': 13, 'reg_alpha': 6.358937832502518, 'reg_lambda': 0.4546808609017141, 'gamma': 2.021901560943972}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:48,626]\u001b[0m Trial 22 finished with value: 0.2919922697750018 and parameters: {'n_estimators': 908, 'learning_rate': 0.015400413364751168, 'max_depth': 12, 'subsample': 0.7973604535488019, 'colsample_bytree': 0.8938862960260908, 'min_child_weight': 11, 'reg_alpha': 3.394955573608009, 'reg_lambda': 1.5555951407631583, 'gamma': 3.3938830438566505}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:49,537]\u001b[0m Trial 23 finished with value: 0.29039497822698335 and parameters: {'n_estimators': 960, 'learning_rate': 0.03095245122491554, 'max_depth': 11, 'subsample': 0.7521346944636222, 'colsample_bytree': 0.8127708387600537, 'min_child_weight': 14, 'reg_alpha': 3.856589526533006, 'reg_lambda': 3.3081156519089987, 'gamma': 1.3134988575441255}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n",
      "\u001b[32m[I 2026-02-17 22:18:52,074]\u001b[0m Trial 24 finished with value: 0.285666261539655 and parameters: {'n_estimators': 889, 'learning_rate': 0.00800272840176958, 'max_depth': 10, 'subsample': 0.8708959720309114, 'colsample_bytree': 0.8723241465332208, 'min_child_weight': 9, 'reg_alpha': 1.4186458189032338, 'reg_lambda': 4.810415224559519, 'gamma': 2.2411615463556704}. Best is trial 18 with value: 0.3239231657373558.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial value: 0.3239231657373558\n",
      "Best params: {'n_estimators': 900, 'learning_rate': 0.014765133323932157, 'max_depth': 11, 'subsample': 0.8124241410164256, 'colsample_bytree': 0.836120955256684, 'min_child_weight': 13, 'reg_alpha': 3.6596656394528524, 'reg_lambda': 4.070937279766028, 'gamma': 0.7701164651327226}\n",
      "Reg QWK: 0.29757578372262705\n"
     ]
    }
   ],
   "source": [
    "# Plan 2: reg with optimized rounder\n",
    "# Hyperparameter-tuning w/Optuna (Regressor)\n",
    "\n",
    "# Spaghetti split \n",
    "X_train_model, X_val_rounder, y_train_model, y_val_rounder = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist', # Faster training\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Use GPU if available\n",
    "        \n",
    "        # Tuning parameters\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000), # More trees, but early stopping handles it\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 15), # Higher = prevent isolating outliers\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10), # Regularizations (a,b,g)\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0), # \n",
    "    }\n",
    "\n",
    "    # Split for early stopping (Optuna needs a validation set)\n",
    "    # Using specific validation set\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train_model, y_train_model, test_size=0.1, random_state=42, \n",
    "                                                stratify=y_train_model\n",
    "                                                )\n",
    "\n",
    "    # Choose regressor if trying to use with optimied rounder\n",
    "    model = xgb.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # FIXED: Round continuous predictions to integers for Kappa calculation\n",
    "    preds = model.predict(X_val)\n",
    "    preds_rounded = np.rint(preds).astype(int).clip(0, 4) \n",
    "    kappa = cohen_kappa_score(y_val, preds_rounded, weights='quadratic')\n",
    "    return kappa\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25) # Run 50 smart trials\n",
    "\n",
    "print(f\"Best trial value: {study.best_value}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Use best params\n",
    "best_params = study.best_params\n",
    "# Add fixed params back\n",
    "best_params['objective'] = 'reg:squarederror'\n",
    "\n",
    "xgb_optuna_reg = xgb.XGBRegressor(**best_params)\n",
    "xgb_optuna_reg.fit(X_train_model, y_train_model) \n",
    "pred_xgb_reg_raw = xgb_optuna_reg.predict(X_test)\n",
    "pred_xgb_reg = np.rint(pred_xgb_reg_raw).astype(int).clip(0, 4)\n",
    "print (\"Reg QWK:\", cohen_kappa_score(pred_xgb_reg, y_test, weights=\"quadratic\"))\n",
    "#joblib.dump(xgb_optuna_reg, 'xgb_optuna_reg_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6421e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from functools import partial\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') # Optimizer\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p.astype(int)\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a15d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: [0.52935692 1.84113145 2.42150447 2.85848287]\n",
      "Optimized QWK: 0.3860\n"
     ]
    }
   ],
   "source": [
    "# Optimize Thresholds\n",
    "pred_xgb_reg_val = xgb_optuna_reg.predict(X_val_rounder)\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(pred_xgb_reg_val, y_val_rounder)\n",
    "res = optR.coefficients()\n",
    "print(f\"Optimized Thresholds: {res}\")\n",
    "\n",
    "# Final Predictions\n",
    "pred_optR = optR.predict(pred_xgb_reg_raw, res)\n",
    "print(f\"Optimized QWK: {cohen_kappa_score(y_test, pred_optR, weights='quadratic'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be4b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal thresholds: [1.6950223279785117, 2.1226930643496984, 2.4799235407383597, 3.035984594650112]\n",
      "Optimal Quadratic kappa: 0.4041\n",
      "Original Quadratic kappa: 0.2976\n"
     ]
    }
   ],
   "source": [
    "from oprounder import OptimizedRounder\n",
    "\n",
    "# for fitting the rounder\n",
    "#pred_train_reg = xgb_optuna_reg.predict(X_train)\n",
    "pred_val_rounder = xgb_optuna_reg.predict(X_val_rounder) \n",
    "\n",
    "# what we want to predict (eval)\n",
    "pred_test_reg = xgb_optuna_reg.predict(X_test)\n",
    "\n",
    "# Fit the Optimized Rounder on Training Data\n",
    "rounder = OptimizedRounder(n_classes=y_train.nunique(), n_trials=100)\n",
    "rounder.fit(pred_val_rounder, y_val_rounder) \n",
    "\n",
    "\n",
    "# View the learned thresholds\n",
    "print(f'Optimal thresholds: {rounder.thresholds}')\n",
    "\n",
    "# Predict on Test Data using the new thresholds\n",
    "pred_reg_optimized = rounder.predict(pred_test_reg) # use the new threshold to pick label\n",
    "\n",
    "# Compare how the new threshold improve kappa\n",
    "kappa = cohen_kappa_score(y_test, pred_reg_optimized, weights='quadratic')\n",
    "print(f'Optimal Quadratic kappa: {kappa:.4f}')\n",
    "\n",
    "kappa = cohen_kappa_score(y_test, pred_xgb_reg, weights='quadratic')\n",
    "print(f'Original Quadratic kappa: {kappa:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ce85c",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156dd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Define categorical features indices\n",
    "cat_feature_names = list(X_train.select_dtypes(include=['category']).columns)\n",
    "print(f\"Categorical features for CatBoost: {cat_feature_names}\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                stratify=y_train\n",
    "                                                )\n",
    "\n",
    "# CatBoost handles the categories automatically (no need for OHE)\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='Kappa',\n",
    "    loss_function='MultiClass',\n",
    "    cat_features=cat_feature_names,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "clf.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "\n",
    "pred_cat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "#joblib.dump(xgb_optuna, 'xgb_v2.pkl')\n",
    "#joblib.dump(xgb_optuna_reg, 'xgb_v2_reg.pkl')\n",
    "#joblib.dump(vec_tuple[0], 'tfidf_vectorizer.pkl')\n",
    "#joblib.dump(vec_tuple[1], 'svd_transformer.pkl')\n",
    "\n",
    "xgb_optuna.save_model(\"xgb_v2.json\") \n",
    "xgb_optuna_reg.save_model(\"xgb_v2_reg.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c887d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "# Testing if loaded models' working\n",
    "model_testing = joblib.load(\"../models/v2/xgb_v2.pkl\")\n",
    "loaded_tfidf = joblib.load(\"../models/v2/tfidf_vectorizer.pkl\")\n",
    "loaded_svd = joblib.load(\"../models/v2/svd_transformer.pkl\")\n",
    "vec_tuple = (loaded_tfidf, loaded_svd)\n",
    "X_testing_raw = X_test_raw.copy()\n",
    "X_testing_text, _ = generate_text_features(X_testing_raw, is_train=False, fit_on_text=vec_tuple)\n",
    "X_testing = featurize_table(X_testing_text)\n",
    "pred_testing = model_testing.predict(X_test)\n",
    "\n",
    "\n",
    "def evaluate_model(model, model_prediction):\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Kappa Score: {cohen_kappa_score(model_prediction, y_test, weights='quadratic'):.4f}\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(model_prediction, y_test):.4f}\")\n",
    "   \n",
    "\n",
    "print (evaluate_model(xgb_optuna, pred_xgb))\n",
    "print(\"\")\n",
    "print (evaluate_model(xgb_optuna_reg, pred_xgb_reg))\n",
    "#print(f'Optimal Quadratic kappa: {cohen_kappa_score(y_test, pred_reg_optimized, weights='quadratic'):.4f}')\n",
    "#print (\"Loaded QWK:\" , cohen_kappa_score(y_test, pred_testing, weights='quadratic'))\n",
    "\n",
    "#print (evaluate_model(clf, pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7805bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple blending example\n",
    "pred_xgb = xgb_optuna.predict_proba(X_test)\n",
    "pred_cat = clf.predict_proba(X_test)\n",
    "\n",
    "# Weighted Average (Give more weight to the better model)\n",
    "final_probs = (0.6 * pred_cat) + (0.4 * pred_xgb)\n",
    "final_preds = np.argmax(final_probs, axis=1)\n",
    "\n",
    "print(f\"Ensemble Kappa: {cohen_kappa_score(y_test, final_preds, weights='quadratic'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56851b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: which factors benefit the prediction the most (extracted from XGBoost)\n",
    "# ensure usage of original feature names not just np array indices\n",
    "\"\"\"\n",
    "xgb_optuna.get_booster().feature_names = list(X_train.columns)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "xgb.plot_importance(xgb_optuna, max_num_features=15, height=0.5, importance_type=\"gain\", values_format = \"{v:.2f}\") # Weight à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¸›à¸£à¸°à¸à¸­à¸š tree, Gain à¸­à¸°à¹„à¸£à¹à¸šà¹ˆà¸‡à¹„à¸”à¹‰à¸¡à¸²à¸à¸ªà¸¸à¸”\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Also plot for the regressor model\n",
    "xgb_optuna_reg.get_booster().feature_names = list(X_train.columns)\n",
    "plt.figure(figsize=(12,6))\n",
    "xgb.plot_importance(xgb_optuna_reg, max_num_features=15, height=0.5, importance_type=\"gain\", values_format=\"{v:.2f}\")\n",
    "plt.title(\"Feature Importance (Gain) - Regressor\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27da00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the classifier model\n",
    "importance_dict = xgb_optuna_reg.get_booster().get_score(importance_type='gain')\n",
    "xgb_optuna_reg.get_booster().feature_names = list(X_train.columns)\n",
    "# Sort by importance (ascending to get the least important features)\n",
    "sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "# Get bottom 5 features\n",
    "bottom_5 = sorted_importance[:5]\n",
    "\n",
    "print(\"Bottom 5 least important features (by gain):\")\n",
    "for feature, importance in bottom_5:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Shit to drop:\", [f for f, _ in bottom_5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
