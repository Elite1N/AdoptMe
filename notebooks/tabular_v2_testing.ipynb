{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d75b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Notebook is for experimenting with features & trying to improve boosting models\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import joblib\n",
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b028d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: forward selection: use only features that improves kappa\n",
    "def featurize_table(data_df):\n",
    "    tabular_df = data_df.copy()\n",
    "    # Namelength\n",
    "    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n",
    "    \n",
    "    # Description length\n",
    "    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    \n",
    "    # Is Mixed Breed? (Breed2 is not 0)\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. Text\n",
    "    tabular_df['word_count'] = tabular_df['Description'].str.split().str.len().fillna(0)\n",
    "    tabular_df['char_count'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    tabular_df['avg_word_len'] = tabular_df['char_count'] / (tabular_df['word_count'] + 1)\n",
    "    tabular_df['num_digits'] = tabular_df['Description'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
    "    tabular_df['all_caps_ratio'] = tabular_df['Description'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(1, len(str(x))))\n",
    "\n",
    "    # 2. Measures\n",
    "    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0,1)\n",
    "    tabular_df['photo_per_pet'] = tabular_df['PhotoAmt'] / tabular_df['Quantity']\n",
    "    tabular_df['age_per_size'] = tabular_df['Age'] / tabular_df['MaturitySize'] # Needs careful handling of 0s\n",
    "    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt'] # Total Media (Engagement proxy)\n",
    "    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1) # Number of Colors (Count non-zero color columns)\n",
    "    \n",
    "    \n",
    "    # 3. Simple Interactions\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0) & (tabular_df['Breed2'].notnull())\n",
    "    tabular_df['is_specific_color'] = (tabular_df['Color2'] != 0) # Has more than 1 color    \n",
    "    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)    # Is Free? (Fee is 0)\n",
    "    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)   # Health Issue Flag (Health > 1 implies injury or condition)\n",
    "    \n",
    "    # log transform for shit and giggles\n",
    "    tabular_df['Fee'] = np.log1p(tabular_df['Fee'])\n",
    "    tabular_df['PhotoAmt'] = np.log1p(tabular_df['PhotoAmt'])\n",
    "    \n",
    "    \n",
    "    # Encode categories\n",
    "    \"\"\"\n",
    "    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n",
    "                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n",
    "                    'Sterilized', 'Health', 'State']\n",
    "    tabular_df[cat_cols] = tabular_df[cat_cols].astype('category')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop useless features\n",
    "    features_to_drop = []\n",
    "    #features_to_drop = ['svd_desc_9', 'Color3', 'VideoAmt', 'name_length', 'sentiment_magnitude','Color3', 'Health', 'num_digits', 'Dewormed', 'sentiment_polarity', 'is_specific_color', 'num_colors', 'Color2', 'svd_desc_11', 'svd_desc_18', 'svd_desc_16', 'svd_desc_14', 'svd_desc_2', 'sentiment_score', 'word_count']\n",
    "    print (\"Dropping\", len(features_to_drop), \"features\")\n",
    "    tabular_df = tabular_df.drop(features_to_drop, axis=1, inplace=False)\n",
    "    # Drop text and ID columns\n",
    "    tabular_df.drop(['Name', 'PetID', 'Description'], axis=1, inplace=True)\n",
    "    return tabular_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c819022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features\n",
    "def extract_sentiment_from_json(pet_id, sentiment_dir=\"../data/train_sentiment/\"):\n",
    "    # This assumes the sentiment files follow the pattern {PetID}.json\n",
    "    filename = f\"{sentiment_dir}/{pet_id}.json\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            # Usually 'documentSentiment' holds the overall score\n",
    "            if 'documentSentiment' in data:\n",
    "                return data['documentSentiment']['score'], data['documentSentiment']['magnitude']\n",
    "    except:\n",
    "        pass\n",
    "    return 0, 0 # Default if missing\n",
    "\n",
    "# TODO: decrease svd bcuz small data\n",
    "def generate_text_features(df, svd_components=5, is_train=True, fit_on_text=None):\n",
    "    \"\"\"\n",
    "    df: The dataframe (containing 'Description' and 'PetID')\n",
    "    svd_components: Number of latent features to keep\n",
    "    is_train: Boolean, used to decide whether to fit or transform\n",
    "    fit_on_text: If is_train=False, pass the vectorizers here (tuple: tfidf, svd)\n",
    "    \"\"\"\n",
    "    df_text = df.copy()\n",
    "    \n",
    "    # 1. TF-IDF + SVD (Latent Semantic Analysis)\n",
    "    print(\"Generating TF-IDF SVD features...\")\n",
    "    descriptions = df_text['Description'].fillna(\"none\").astype(str)\n",
    "    \n",
    "    if is_train:\n",
    "        # Fit on TRAINING descriptions\n",
    "        tfidf = TfidfVectorizer(min_df=3,  max_features=1000, \n",
    "                                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                                ngram_range=(1, 3), use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                                stop_words = 'english')\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "        \n",
    "        # Fit Transform\n",
    "        tf_vecs = tfidf.fit_transform(descriptions)\n",
    "        svd_vecs = svd.fit_transform(tf_vecs)\n",
    "        \n",
    "        # Save vectorizers for inference later\n",
    "        vectorizers = (tfidf, svd)\n",
    "    else:\n",
    "        # Load from passed tuple\n",
    "        tfidf, svd = fit_on_text\n",
    "        tf_vecs = tfidf.transform(descriptions)\n",
    "        svd_vecs = svd.transform(tf_vecs)\n",
    "        vectorizers = fit_on_text\n",
    "\n",
    "    # Create Columns\n",
    "    svd_df = pd.DataFrame(svd_vecs, columns=[f'svd_desc_{i}' for i in range(svd_components)])\n",
    "    # We reset index to make sure concat aligns correctly row-by-row\n",
    "    df_text = pd.concat([df_text.reset_index(drop=True), svd_df], axis=1)\n",
    "\n",
    "    # 2. Sentiment Analysis (File-based lookup)\n",
    "    # Determine directory\n",
    "    sent_dir = \"../data/train_sentiment\" if is_train else \"../data/test_sentiment\"\n",
    "    \n",
    "    print(\"Extracting Sentiment...\")\n",
    "    # Apply row-wise (can be slow, maybe parallelize with pandarallel if needed)\n",
    "    sent_data = df_text['PetID'].apply(lambda x: extract_sentiment_from_json(x, sent_dir))\n",
    "    \n",
    "    df_text['sentiment_score'] = [x[0] for x in sent_data]\n",
    "    df_text['sentiment_magnitude'] = [x[1] for x in sent_data]\n",
    "    df_text['sentiment_polarity'] = df_text['sentiment_score'] * df_text['sentiment_magnitude']\n",
    "\n",
    "    return df_text, vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8be96e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF SVD features...\n",
      "Extracting Sentiment...\n",
      "Generating TF-IDF SVD features...\n",
      "Extracting Sentiment...\n",
      "Dropping 0 features\n",
      "Dropping 0 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>svd_desc_0</th>\n",
       "      <th>svd_desc_1</th>\n",
       "      <th>svd_desc_2</th>\n",
       "      <th>svd_desc_3</th>\n",
       "      <th>svd_desc_4</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>name_length</th>\n",
       "      <th>description_length</th>\n",
       "      <th>is_mixed_breed</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>num_digits</th>\n",
       "      <th>all_caps_ratio</th>\n",
       "      <th>fee_per_pet</th>\n",
       "      <th>photo_per_pet</th>\n",
       "      <th>age_per_size</th>\n",
       "      <th>total_media</th>\n",
       "      <th>num_colors</th>\n",
       "      <th>is_specific_color</th>\n",
       "      <th>is_free</th>\n",
       "      <th>has_health_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>ba248f761903dcd4c4342cc724a52145</td>\n",
       "      <td>0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.244532</td>\n",
       "      <td>-0.037003</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.125729</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>17.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>True</td>\n",
       "      <td>80.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>744fa4278196568a18fcf5cd7d324ed2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.129967</td>\n",
       "      <td>-0.014343</td>\n",
       "      <td>-0.011156</td>\n",
       "      <td>0.216831</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.923077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>4d2400be2e2e78265f5c84345b7a3415</td>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.036106</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>-0.041622</td>\n",
       "      <td>0.041715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>b53c34474d9e24574bcec6a3d3306a0d</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.230322</td>\n",
       "      <td>0.946173</td>\n",
       "      <td>0.060399</td>\n",
       "      <td>0.014193</td>\n",
       "      <td>-0.104804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>b953d651238f379c63e732925f71a5a2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.136872</td>\n",
       "      <td>0.353946</td>\n",
       "      <td>-0.019203</td>\n",
       "      <td>-0.115202</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11989</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>265</td>\n",
       "      <td>299</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>bc599c86ccd17d15a1c758b12d7e851b</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>-0.049383</td>\n",
       "      <td>-0.056765</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.039277</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.05</td>\n",
       "      <td>6.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>True</td>\n",
       "      <td>66.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>5.074627</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11990</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>48d06353f65ac65dd35a8875b70962c5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.224185</td>\n",
       "      <td>-0.049285</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.107381</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>6.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>False</td>\n",
       "      <td>63.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.890625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11991</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41401</td>\n",
       "      <td>a52ad34e621f25688c3a0a579c31ca4d</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.161576</td>\n",
       "      <td>-0.049220</td>\n",
       "      <td>0.056163</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>True</td>\n",
       "      <td>50.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>5.568627</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11992</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41326</td>\n",
       "      <td>95481e953f8aed9ec3d16fc4509537e8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.163324</td>\n",
       "      <td>-0.050172</td>\n",
       "      <td>-0.018623</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>True</td>\n",
       "      <td>42.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.093023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11993</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>299</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41401</td>\n",
       "      <td>c026c581cda87aee6a1b629cfb54ef36</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.302058</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>-0.043018</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>-0.067099</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>True</td>\n",
       "      <td>66.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>5.179104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11994 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
       "0         1    2     307     307       1       1       0       0   \n",
       "1         1   12     307       0       2       2       0       0   \n",
       "2         1    2     307     307       1       1       2       7   \n",
       "3         1    2     307     307       1       3       0       0   \n",
       "4         2    4     265       0       3       1       2       0   \n",
       "...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "11989     2    8     265     299       2       1       2       0   \n",
       "11990     1    2     307       0       2       6       7       0   \n",
       "11991     1    2     307     307       1       1       0       0   \n",
       "11992     1    2     307     307       1       1       2       7   \n",
       "11993     2   30     299     266       1       3       4       5   \n",
       "\n",
       "       MaturitySize  FurLength  Vaccinated  Dewormed  Sterilized  Health  \\\n",
       "0                 2          2           2         1           2       1   \n",
       "1                 1          1           1         1           1       1   \n",
       "2                 2          1           2         2           2       1   \n",
       "3                 2          2           1         1           2       1   \n",
       "4                 2          2           2         2           2       1   \n",
       "...             ...        ...         ...       ...         ...     ...   \n",
       "11989             2          2           1         1           2       1   \n",
       "11990             1          1           2         1           3       1   \n",
       "11991             2          2           1         1           2       1   \n",
       "11992             2          2           2         2           2       1   \n",
       "11993             2          1           1         1           1       1   \n",
       "\n",
       "       Quantity  Fee  State                         RescuerID  VideoAmt  \\\n",
       "0             3  0.0  41326  ba248f761903dcd4c4342cc724a52145         0   \n",
       "1             1  0.0  41326  744fa4278196568a18fcf5cd7d324ed2         0   \n",
       "2             1  0.0  41326  4d2400be2e2e78265f5c84345b7a3415         0   \n",
       "3             1  0.0  41326  b53c34474d9e24574bcec6a3d3306a0d         0   \n",
       "4             4  0.0  41326  b953d651238f379c63e732925f71a5a2         0   \n",
       "...         ...  ...    ...                               ...       ...   \n",
       "11989         1  0.0  41326  bc599c86ccd17d15a1c758b12d7e851b         0   \n",
       "11990         1  0.0  41326  48d06353f65ac65dd35a8875b70962c5         0   \n",
       "11991         1  0.0  41401  a52ad34e621f25688c3a0a579c31ca4d         0   \n",
       "11992         1  0.0  41326  95481e953f8aed9ec3d16fc4509537e8         0   \n",
       "11993         1  0.0  41401  c026c581cda87aee6a1b629cfb54ef36         0   \n",
       "\n",
       "       PhotoAmt  svd_desc_0  svd_desc_1  svd_desc_2  svd_desc_3  svd_desc_4  \\\n",
       "0      2.197225    0.244532   -0.037003    0.009080    0.022057    0.125729   \n",
       "1      1.386294    0.129967   -0.014343   -0.011156    0.216831   -0.013246   \n",
       "2      0.693147    0.036106    0.006342    0.009166   -0.041622    0.041715   \n",
       "3      1.098612    0.230322    0.946173    0.060399    0.014193   -0.104804   \n",
       "4      1.098612    0.136872    0.353946   -0.019203   -0.115202   -0.007164   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "11989  1.098612    0.205444   -0.049383   -0.056765    0.033504    0.039277   \n",
       "11990  0.693147    0.224185   -0.049285    0.025776    0.000251    0.107381   \n",
       "11991  1.098612    0.161576   -0.049220    0.056163    0.033891   -0.002027   \n",
       "11992  1.098612    0.163324   -0.050172   -0.018623    0.013714    0.013797   \n",
       "11993  1.098612    0.302058    0.024008   -0.043018    0.068040   -0.067099   \n",
       "\n",
       "       sentiment_score  sentiment_magnitude  sentiment_polarity  name_length  \\\n",
       "0                  0.3                  1.0                0.30         17.0   \n",
       "1                  0.6                  1.8                1.08          5.0   \n",
       "2                  0.0                  0.0                0.00         11.0   \n",
       "3                  0.0                  0.0                0.00          6.0   \n",
       "4                  0.2                  0.2                0.04         11.0   \n",
       "...                ...                  ...                 ...          ...   \n",
       "11989              0.5                  2.1                1.05          6.0   \n",
       "11990              0.2                  2.1                0.42          6.0   \n",
       "11991              0.4                  1.9                0.76          8.0   \n",
       "11992              0.3                  1.4                0.42          5.0   \n",
       "11993              0.3                  2.6                0.78          6.0   \n",
       "\n",
       "       description_length  is_mixed_breed  word_count  char_count  \\\n",
       "0                   450.0            True        80.0       450.0   \n",
       "1                    77.0           False        12.0        77.0   \n",
       "2                    29.0            True         5.0        29.0   \n",
       "3                    12.0            True         2.0        12.0   \n",
       "4                    20.0           False         3.0        20.0   \n",
       "...                   ...             ...         ...         ...   \n",
       "11989               340.0            True        66.0       340.0   \n",
       "11990               313.0           False        63.0       313.0   \n",
       "11991               284.0            True        50.0       284.0   \n",
       "11992               219.0            True        42.0       219.0   \n",
       "11993               347.0            True        66.0       347.0   \n",
       "\n",
       "       avg_word_len  num_digits  all_caps_ratio  fee_per_pet  photo_per_pet  \\\n",
       "0          5.555556           4        0.008889          0.0       2.666667   \n",
       "1          5.923077           0        0.038961          0.0       3.000000   \n",
       "2          4.833333           0        0.137931          0.0       1.000000   \n",
       "3          4.000000           0        0.083333          0.0       2.000000   \n",
       "4          5.000000           0        0.050000          0.0       0.500000   \n",
       "...             ...         ...             ...          ...            ...   \n",
       "11989      5.074627           1        0.014706          0.0       2.000000   \n",
       "11990      4.890625           1        0.015974          0.0       1.000000   \n",
       "11991      5.568627           0        0.021127          0.0       2.000000   \n",
       "11992      5.093023           0        0.027397          0.0       2.000000   \n",
       "11993      5.179104           1        0.025937          0.0       2.000000   \n",
       "\n",
       "       age_per_size  total_media  num_colors  is_specific_color  is_free  \\\n",
       "0               1.0          8.0           1              False        1   \n",
       "1              12.0          3.0           1              False        1   \n",
       "2               1.0          1.0           3               True        1   \n",
       "3               1.0          2.0           1              False        1   \n",
       "4               2.0          2.0           2               True        1   \n",
       "...             ...          ...         ...                ...      ...   \n",
       "11989           4.0          2.0           2               True        1   \n",
       "11990           2.0          1.0           2               True        1   \n",
       "11991           1.0          2.0           1              False        1   \n",
       "11992           1.0          2.0           3               True        1   \n",
       "11993          15.0          2.0           3               True        1   \n",
       "\n",
       "       has_health_issue  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "11989                 0  \n",
       "11990                 0  \n",
       "11991                 0  \n",
       "11992                 0  \n",
       "11993                 0  \n",
       "\n",
       "[11994 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "full_df = pd.read_csv(\"../data/train/train.csv\") #Beware of directory\n",
    "\n",
    "# Splitting the data from train.csv\n",
    "X = full_df.drop(['AdoptionSpeed'], axis=1)\n",
    "y = full_df['AdoptionSpeed'] \n",
    "X_train_raw, X_eval_raw, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Generate  Features\n",
    "X_train_text, vec_tuple = generate_text_features(X_train_raw, is_train=True)\n",
    "X_eval_text, _ = generate_text_features(X_eval_raw,is_train=False, fit_on_text=vec_tuple)\n",
    "\n",
    "\n",
    "X_train = featurize_table(X_train_text)\n",
    "X_eval = featurize_table(X_eval_text)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc83a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Calculate weights inversely proportional to class frequencies\\nsample_weights = compute_sample_weight(class_weight=\\'balanced\\', y=y_train)\\ndef objective(trial):\\n    params = {\\n        # Config for how to predict\\n        \\'objective\\': \\'multi:softprob\\',\\n        \\'eval_metric\\': \\'mlogloss\\',\\n        \\'num_class\\': 5,\\n        \\'tree_method\\': \\'hist\\', # Faster training\\n        #\\'enable_categorical\\': True,\\n        \\'device\\': \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\', # Use GPU if available\\n\\n        # Tuning parameters\\n        \\'n_estimators\\': trial.suggest_int(\\'n_estimators\\', 200, 1000), # More trees, but early stopping handles it\\n        \\'learning_rate\\': trial.suggest_float(\\'learning_rate\\', 0.005, 0.2, log=True),\\n        \\'max_depth\\': trial.suggest_int(\\'max_depth\\', 3, 12),\\n        \\'subsample\\': trial.suggest_float(\\'subsample\\', 0.6, 1.0),\\n        \\'colsample_bytree\\': trial.suggest_float(\\'colsample_bytree\\', 0.6, 1.0), # % of feature used per tree\\n        \\'min_child_weight\\': trial.suggest_int(\\'min_child_weight\\', 1, 10),\\n        \\'reg_alpha\\': trial.suggest_float(\\'reg_alpha\\', 0, 10),\\n        \\'reg_lambda\\': trial.suggest_float(\\'reg_lambda\\', 0, 10),\\n    }\\n\\n    # Split for early stopping (Optuna needs a validation set)\\n    # Using specific validation set (futher split from train set)\\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \\n                                                stratify=y_train\\n                                                )\\n\\n    # Choose regressor if trying to use with optimied rounder\\n    model = xgb.XGBClassifier(**params, early_stopping_rounds=50)\\n\\n    model.fit(\\n        X_tr, y_tr,\\n        eval_set=[(X_val, y_val)],\\n        verbose=False\\n    )\\n\\n    preds = model.predict(X_val)\\n    kappa = cohen_kappa_score(y_val, preds, weights=\\'quadratic\\')\\n    return kappa\\n\\nstudy = optuna.create_study(direction=\\'maximize\\')\\nstudy.optimize(objective, n_trials=50) # Run 50 smart trials\\n\\nprint(f\"Best trial value: {study.best_value}\")\\nprint(f\"Best params: {study.best_params}\")\\n\\n# Use best params\\nbest_params = study.best_params\\n# Add fixed params back\\nbest_params[\\'objective\\'] = \\'multi:softprob\\'\\nbest_params[\\'num_class\\'] = 5\\n\\nxgb_optuna = xgb.XGBClassifier(**best_params)\\nxgb_optuna.fit(X_train, y_train, sample_weight=sample_weights) \\npred_xgb = xgb_optuna.predict(X_test)\\n#joblib.dump(xgb_optuna, \\'xgb_optuna_full.pkl\\')\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plan 1: normal classification\n",
    "# Hyperparameter-tuning w/Optuna\n",
    "\"\"\"\n",
    "# Calculate weights inversely proportional to class frequencies\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # Config for how to predict\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 5,\n",
    "        'tree_method': 'hist', # Faster training\n",
    "        #'enable_categorical': True,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Use GPU if available\n",
    "        \n",
    "        # Tuning parameters\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000), # More trees, but early stopping handles it\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0), # % of feature used per tree\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "    }\n",
    "\n",
    "    # Split for early stopping (Optuna needs a validation set)\n",
    "    # Using specific validation set (futher split from train set)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                stratify=y_train\n",
    "                                                )\n",
    "\n",
    "    # Choose regressor if trying to use with optimied rounder\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=50)\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    kappa = cohen_kappa_score(y_val, preds, weights='quadratic')\n",
    "    return kappa\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50) # Run 50 smart trials\n",
    "\n",
    "print(f\"Best trial value: {study.best_value}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Use best params\n",
    "best_params = study.best_params\n",
    "# Add fixed params back\n",
    "best_params['objective'] = 'multi:softprob'\n",
    "best_params['num_class'] = 5\n",
    "\n",
    "xgb_optuna = xgb.XGBClassifier(**best_params)\n",
    "xgb_optuna.fit(X_train, y_train, sample_weight=sample_weights) \n",
    "pred_xgb = xgb_optuna.predict(X_test)\n",
    "#joblib.dump(xgb_optuna, 'xgb_optuna_full.pkl')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c30a5e",
   "metadata": {},
   "source": [
    "XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50c9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-18 05:11:30,446]\u001b[0m A new study created in memory with name: no-name-87913d75-d1a6-4d8d-a791-dbc0415b1011\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:11:36,009]\u001b[0m Trial 0 finished with value: 0.244375343013171 and parameters: {'n_estimators': 992, 'learning_rate': 0.005072180964762284, 'max_depth': 3, 'subsample': 0.7170060240138587, 'colsample_bytree': 0.6308962850887303, 'min_child_weight': 11, 'reg_alpha': 5.213591924603724, 'reg_lambda': 7.0156860095858855, 'gamma': 0.0489982839126879}. Best is trial 0 with value: 0.244375343013171.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:11:42,383]\u001b[0m Trial 1 finished with value: 0.2593623069106171 and parameters: {'n_estimators': 999, 'learning_rate': 0.007581658626109493, 'max_depth': 5, 'subsample': 0.8085953208066867, 'colsample_bytree': 0.8246705885156356, 'min_child_weight': 3, 'reg_alpha': 5.174355588552064, 'reg_lambda': 9.102319978755805, 'gamma': 0.12159376200198913}. Best is trial 1 with value: 0.2593623069106171.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:11:47,170]\u001b[0m Trial 2 finished with value: 0.24631304307085583 and parameters: {'n_estimators': 625, 'learning_rate': 0.005838616927215505, 'max_depth': 5, 'subsample': 0.621238554787316, 'colsample_bytree': 0.7253860988479727, 'min_child_weight': 12, 'reg_alpha': 3.0008033660435154, 'reg_lambda': 4.640383728767592, 'gamma': 1.7582752705294276}. Best is trial 1 with value: 0.2593623069106171.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:11:51,679]\u001b[0m Trial 3 finished with value: 0.26368248814775525 and parameters: {'n_estimators': 1988, 'learning_rate': 0.015467522378762023, 'max_depth': 3, 'subsample': 0.8983620635993037, 'colsample_bytree': 0.7100176758935688, 'min_child_weight': 14, 'reg_alpha': 6.041781865966442, 'reg_lambda': 0.30072381144873117, 'gamma': 3.0722718797137}. Best is trial 3 with value: 0.26368248814775525.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:11:54,180]\u001b[0m Trial 4 finished with value: 0.26058385936414946 and parameters: {'n_estimators': 544, 'learning_rate': 0.041760964688342055, 'max_depth': 3, 'subsample': 0.7165272430763868, 'colsample_bytree': 0.9557089594588283, 'min_child_weight': 13, 'reg_alpha': 9.505658423752394, 'reg_lambda': 1.5212057068986369, 'gamma': 4.90858123450118}. Best is trial 3 with value: 0.26368248814775525.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:00,474]\u001b[0m Trial 5 finished with value: 0.2606334719373969 and parameters: {'n_estimators': 1615, 'learning_rate': 0.008845903528037265, 'max_depth': 4, 'subsample': 0.8313481228516071, 'colsample_bytree': 0.9592585661514021, 'min_child_weight': 9, 'reg_alpha': 5.233347125184296, 'reg_lambda': 4.759462938282248, 'gamma': 0.2829468389692319}. Best is trial 3 with value: 0.26368248814775525.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:05,962]\u001b[0m Trial 6 finished with value: 0.25748516641592617 and parameters: {'n_estimators': 1860, 'learning_rate': 0.009473558132679347, 'max_depth': 5, 'subsample': 0.8639278998809352, 'colsample_bytree': 0.8564399153876634, 'min_child_weight': 11, 'reg_alpha': 3.441399720597298, 'reg_lambda': 2.1162262690638802, 'gamma': 2.503539816679253}. Best is trial 3 with value: 0.26368248814775525.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:07,814]\u001b[0m Trial 7 finished with value: 0.2690389868347194 and parameters: {'n_estimators': 1934, 'learning_rate': 0.08480373506071845, 'max_depth': 3, 'subsample': 0.7124216731227128, 'colsample_bytree': 0.8544160577816005, 'min_child_weight': 5, 'reg_alpha': 4.096916542113758, 'reg_lambda': 5.188381347476486, 'gamma': 3.906169495880871}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:13,901]\u001b[0m Trial 8 finished with value: 0.25941884248923397 and parameters: {'n_estimators': 1471, 'learning_rate': 0.008863023381151691, 'max_depth': 5, 'subsample': 0.7217160392233426, 'colsample_bytree': 0.8406966241411529, 'min_child_weight': 8, 'reg_alpha': 4.281596328548387, 'reg_lambda': 7.344876396165934, 'gamma': 0.9125012708219582}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:15,196]\u001b[0m Trial 9 finished with value: 0.2616658822002244 and parameters: {'n_estimators': 858, 'learning_rate': 0.1832350511569758, 'max_depth': 5, 'subsample': 0.8425057136667367, 'colsample_bytree': 0.8395010628544934, 'min_child_weight': 9, 'reg_alpha': 6.180369709613571, 'reg_lambda': 1.2959293100749285, 'gamma': 2.7283200022972403}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:16,631]\u001b[0m Trial 10 finished with value: 0.25673277437322306 and parameters: {'n_estimators': 1367, 'learning_rate': 0.09292200101778349, 'max_depth': 4, 'subsample': 0.9945132131477911, 'colsample_bytree': 0.917152293156369, 'min_child_weight': 4, 'reg_alpha': 0.7723989972900154, 'reg_lambda': 3.6531756353714737, 'gamma': 4.194840801733338}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:20,134]\u001b[0m Trial 11 finished with value: 0.261077196489286 and parameters: {'n_estimators': 1962, 'learning_rate': 0.024295041102886657, 'max_depth': 3, 'subsample': 0.9285932955212342, 'colsample_bytree': 0.7325857447117794, 'min_child_weight': 15, 'reg_alpha': 8.839318396889581, 'reg_lambda': 6.62671055968321, 'gamma': 3.558950344626583}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:24,258]\u001b[0m Trial 12 finished with value: 0.2643289120763622 and parameters: {'n_estimators': 1729, 'learning_rate': 0.021088204386189763, 'max_depth': 3, 'subsample': 0.6118139670013891, 'colsample_bytree': 0.7329878496738195, 'min_child_weight': 6, 'reg_alpha': 8.301829877456257, 'reg_lambda': 3.0091013332001895, 'gamma': 3.3879400067724674}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:27,158]\u001b[0m Trial 13 finished with value: 0.26458132620012625 and parameters: {'n_estimators': 1666, 'learning_rate': 0.055074935837875136, 'max_depth': 4, 'subsample': 0.6103000847162691, 'colsample_bytree': 0.770953653331993, 'min_child_weight': 6, 'reg_alpha': 7.899717687824902, 'reg_lambda': 3.697422455550852, 'gamma': 3.8414758331350867}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:30,304]\u001b[0m Trial 14 finished with value: 0.2605729868818487 and parameters: {'n_estimators': 1633, 'learning_rate': 0.050527614746748155, 'max_depth': 4, 'subsample': 0.6664026017829899, 'colsample_bytree': 0.7822259541343037, 'min_child_weight': 6, 'reg_alpha': 7.372631945478332, 'reg_lambda': 5.858185503249107, 'gamma': 4.342240627367339}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:32,939]\u001b[0m Trial 15 finished with value: 0.26068579697468125 and parameters: {'n_estimators': 1239, 'learning_rate': 0.07572068004358448, 'max_depth': 4, 'subsample': 0.7590536430393207, 'colsample_bytree': 0.9061915840763483, 'min_child_weight': 6, 'reg_alpha': 1.524480573919674, 'reg_lambda': 8.590447888775165, 'gamma': 4.222333253100846}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:35,398]\u001b[0m Trial 16 finished with value: 0.2599186865661257 and parameters: {'n_estimators': 1780, 'learning_rate': 0.14640043560881927, 'max_depth': 4, 'subsample': 0.6553161018725376, 'colsample_bytree': 0.7830443778015034, 'min_child_weight': 4, 'reg_alpha': 7.1308958650980685, 'reg_lambda': 3.6890220551047523, 'gamma': 1.9647110018137606}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:38,129]\u001b[0m Trial 17 finished with value: 0.26716300698284356 and parameters: {'n_estimators': 1541, 'learning_rate': 0.08152347226128696, 'max_depth': 3, 'subsample': 0.7682672222821347, 'colsample_bytree': 0.6663463455431965, 'min_child_weight': 7, 'reg_alpha': 2.6246587359783664, 'reg_lambda': 5.589135172733736, 'gamma': 3.667755548969386}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:40,556]\u001b[0m Trial 18 finished with value: 0.25641308213244385 and parameters: {'n_estimators': 1466, 'learning_rate': 0.12205659072087051, 'max_depth': 3, 'subsample': 0.759445437441288, 'colsample_bytree': 0.6004110742384832, 'min_child_weight': 8, 'reg_alpha': 2.2194799788176027, 'reg_lambda': 5.724950853319749, 'gamma': 4.764601792380431}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:42,625]\u001b[0m Trial 19 finished with value: 0.26299852895598613 and parameters: {'n_estimators': 1112, 'learning_rate': 0.08727553685015792, 'max_depth': 3, 'subsample': 0.7695780275681541, 'colsample_bytree': 0.6716941213979178, 'min_child_weight': 5, 'reg_alpha': 0.453162692961949, 'reg_lambda': 7.875167500657989, 'gamma': 3.137242475257824}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:45,245]\u001b[0m Trial 20 finished with value: 0.2642642789412152 and parameters: {'n_estimators': 1513, 'learning_rate': 0.035289550086111884, 'max_depth': 3, 'subsample': 0.6796522658279521, 'colsample_bytree': 0.8902336787763441, 'min_child_weight': 7, 'reg_alpha': 3.809907990633727, 'reg_lambda': 9.983279329309042, 'gamma': 1.8697857456943223}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:47,069]\u001b[0m Trial 21 finished with value: 0.2657861014672845 and parameters: {'n_estimators': 1833, 'learning_rate': 0.06009458717555906, 'max_depth': 4, 'subsample': 0.6858170713453561, 'colsample_bytree': 0.6714553556444138, 'min_child_weight': 7, 'reg_alpha': 2.5463293769427597, 'reg_lambda': 5.526917466509119, 'gamma': 3.7793097780314615}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:48,810]\u001b[0m Trial 22 finished with value: 0.26224944918366166 and parameters: {'n_estimators': 1851, 'learning_rate': 0.06624002495428985, 'max_depth': 4, 'subsample': 0.6997986882199839, 'colsample_bytree': 0.6768462300779037, 'min_child_weight': 9, 'reg_alpha': 2.165827947632575, 'reg_lambda': 5.646290743898107, 'gamma': 3.791228663063715}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:50,400]\u001b[0m Trial 23 finished with value: 0.2623809306000762 and parameters: {'n_estimators': 1888, 'learning_rate': 0.1140911489955901, 'max_depth': 3, 'subsample': 0.7895283893024365, 'colsample_bytree': 0.670327811003327, 'min_child_weight': 7, 'reg_alpha': 2.645477835513012, 'reg_lambda': 4.74941336676883, 'gamma': 4.5485532324218525}. Best is trial 7 with value: 0.2690389868347194.\u001b[0m\n",
      "\u001b[32m[I 2026-02-18 05:12:52,449]\u001b[0m Trial 24 finished with value: 0.2699798635356395 and parameters: {'n_estimators': 1757, 'learning_rate': 0.0583429663217349, 'max_depth': 3, 'subsample': 0.6480151830745275, 'colsample_bytree': 0.6394123307818426, 'min_child_weight': 3, 'reg_alpha': 1.6198673843535316, 'reg_lambda': 6.539737725211661, 'gamma': 3.9203654525127636}. Best is trial 24 with value: 0.2699798635356395.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial value: 0.2699798635356395\n",
      "Best params: {'n_estimators': 1757, 'learning_rate': 0.0583429663217349, 'max_depth': 3, 'subsample': 0.6480151830745275, 'colsample_bytree': 0.6394123307818426, 'min_child_weight': 3, 'reg_alpha': 1.6198673843535316, 'reg_lambda': 6.539737725211661, 'gamma': 3.9203654525127636}\n"
     ]
    }
   ],
   "source": [
    "# Plan 2: reg with optimized rounder\n",
    "# Hyperparameter-tuning w/Optuna (Regressor)\n",
    "groups = X_train['RescuerID']\n",
    "X_train_features = X_train.drop(['RescuerID'], axis=1)\n",
    "X_eval_features = X_eval.drop(['RescuerID'], axis=1)\n",
    "# Spaghetti split  (1 for xgb 1 for rounder)\n",
    "#X_train_model, X_val_rounder, y_train_model, y_val_rounder = train_test_split(X_train_features, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist', # Faster training\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Use GPU if available\n",
    "        \n",
    "        # Tuning parameters\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000), # More trees, but early stopping handles it\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 15), # Higher = prevent isolating outliers\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10), # Regularizations (a,b,g)\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0), # \n",
    "    }\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    kappa_scores = []\n",
    "\n",
    "    for train_idx, val_idx in sgkf.split(X_train_features, y_train, groups=groups):\n",
    "        X_tr_fold = X_train_features.iloc[train_idx]\n",
    "        y_tr_fold = y_train.iloc[train_idx]\n",
    "        \n",
    "        X_val_fold = X_train_features.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr_fold, y_tr_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        preds_rounded = np.rint(preds).astype(int).clip(0, 4)\n",
    "        fold_kappa = cohen_kappa_score(y_val_fold, preds_rounded, weights='quadratic')\n",
    "        kappa_scores.append(fold_kappa)\n",
    "    \n",
    "    return np.mean(kappa_scores)\n",
    "\n",
    "# 1. Run Optuna to get best parameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25) # Run 50 smart trials\n",
    "\n",
    "print(f\"Best trial value: {study.best_value}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Use best params\n",
    "best_params = study.best_params\n",
    "# Add fixed params back\n",
    "best_params['objective'] = 'reg:squarederror'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e43ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Final Models...\n",
      "Saved model for Fold 0\n",
      "Saved model for Fold 1\n",
      "Saved model for Fold 2\n",
      "Saved model for Fold 3\n",
      "Saved model for Fold 4\n",
      "Reg QWK: 0.27019001746534055\n"
     ]
    }
   ],
   "source": [
    "xgb_optuna_reg = xgb.XGBRegressor(**best_params, early_stopping_rounds=50)\n",
    "\n",
    "# 2. FINAL TRAINING with StratifiedGroupKFold\n",
    "# We retrain on 5 folds to get OOF predictions for the Rounder\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X_train)) #TODO: change the origin\n",
    "test_preds_list = []\n",
    "\n",
    "# Directory\n",
    "save_dir = \"../models/v2_stratify\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Training Final Models...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_features, y_train, groups=groups)):\n",
    "    X_tr, y_tr = X_train_features.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    X_val, y_val = X_train_features.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train fold model\n",
    "    model = xgb.XGBRegressor(**best_params, early_stopping_rounds=50)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # Save Model\n",
    "    model.save_model(f\"{save_dir}/xgb_fold_{fold}.json\")\n",
    "    print(f\"Saved model for Fold {fold}\")\n",
    "    \n",
    "    # Save OOF preds (for Rounder optimization)\n",
    "    # We must map pandas index back to position for oof array\n",
    "    val_pos_idx = X_train.index.get_indexer(X_val.index) #TODO: change the origin\n",
    "    oof_preds[val_pos_idx] = model.predict(X_val)\n",
    "    \n",
    "    # Predict on unseen test set (average later)\n",
    "    test_preds_list.append(model.predict(X_eval_features))\n",
    "\n",
    "# Average the test predictions from all 5 models (Bagging)\n",
    "pred_xgb_reg_raw = np.mean(test_preds_list, axis=0) # This is your final raw prediction\n",
    "\n",
    "# 3. Rename variables for the next cells to work\n",
    "pred_xgb_reg_val = oof_preds # These are your \"clean\" validation predictions\n",
    "y_val_rounder = y_train    # The labels matching your OOF preds\n",
    "\n",
    "# Round for evaluation\n",
    "pred_xgb_reg_rounded = np.rint(pred_xgb_reg_val).astype(int).clip(0, 4)\n",
    "print (\"Reg QWK:\", cohen_kappa_score(pred_xgb_reg_rounded, y_train, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6421e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from functools import partial\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') # Optimizer\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p.astype(int)\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a15d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: [0.44387645 2.0385008  2.48836424 2.7660213 ]\n",
      "Optimized QWK: 0.3743\n"
     ]
    }
   ],
   "source": [
    "# Optimize Thresholds\n",
    "pred_eval_raw = model.predict(X_eval_features)\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(pred_xgb_reg_val, y_val_rounder)\n",
    "res = optR.coefficients()\n",
    "print(f\"Optimized Thresholds: {res}\")\n",
    "\n",
    "# Final Predictions\n",
    "pred_eval = optR.predict(pred_eval_raw, res)\n",
    "print(f\"Optimized QWK: {cohen_kappa_score(y_eval, pred_eval, weights='quadratic'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be4b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal thresholds: [1.8248840243475104, 2.059340081076014, 2.5441489214855713, 2.7531737285340787]\n",
      "Optimal Quadratic kappa: 0.3900\n",
      "Original Quadratic kappa: 0.3743\n"
     ]
    }
   ],
   "source": [
    "from oprounder import OptimizedRounder\n",
    "\n",
    "# for fitting the rounder\n",
    "#pred_train_reg = xgb_optuna_reg.predict(X_train)\n",
    "#pred_val_rounder = xgb_optuna_reg.predict(pred_xgb_reg_val) \n",
    "\n",
    "# what we want to predict (eval)\n",
    "pred_eval_reg = model.predict(X_eval_features)\n",
    "\n",
    "# Fit the Optimized Rounder on Training Data\n",
    "rounder = OptimizedRounder(n_classes=y_train.nunique(), n_trials=100)\n",
    "rounder.fit(oof_preds, y_val_rounder) \n",
    "\n",
    "\n",
    "# View the learned thresholds\n",
    "print(f'Optimal thresholds: {rounder.thresholds}')\n",
    "\n",
    "# Predict on Test Data using the new thresholds\n",
    "pred_reg_optimized = rounder.predict(pred_eval_reg) # use the new threshold to pick label\n",
    "\n",
    "# Compare how the new threshold improve kappa\n",
    "kappa = cohen_kappa_score(y_eval, pred_reg_optimized, weights='quadratic')\n",
    "print(f'Optimal Quadratic kappa: {kappa:.4f}')\n",
    "\n",
    "kappa = cohen_kappa_score(y_eval, np.rint(pred_eval).astype(int).clip(0, 4), weights='quadratic')\n",
    "print(f'Original Quadratic kappa: {kappa:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ce85c",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156dd547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom catboost import CatBoostClassifier, Pool\\n\\n# Define categorical features indices\\ncat_feature_names = list(X_train.select_dtypes(include=[\\'category\\']).columns)\\nprint(f\"Categorical features for CatBoost: {cat_feature_names}\")\\n\\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \\n                                                stratify=y_train\\n                                                )\\n\\n# CatBoost handles the categories automatically (no need for OHE)\\nclf = CatBoostClassifier(\\n    iterations=1000,\\n    learning_rate=0.05,\\n    eval_metric=\\'Kappa\\',\\n    loss_function=\\'MultiClass\\',\\n    cat_features=cat_feature_names,\\n    verbose=100\\n)\\n\\nclf.fit(X_tr, y_tr, eval_set=(X_val, y_val))\\n\\npred_cat = clf.predict(X_eval)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Define categorical features indices\n",
    "cat_feature_names = list(X_train.select_dtypes(include=['category']).columns)\n",
    "print(f\"Categorical features for CatBoost: {cat_feature_names}\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                stratify=y_train\n",
    "                                                )\n",
    "\n",
    "# CatBoost handles the categories automatically (no need for OHE)\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='Kappa',\n",
    "    loss_function='MultiClass',\n",
    "    cat_features=cat_feature_names,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "clf.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "\n",
    "pred_cat = clf.predict(X_eval)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "#joblib.dump(xgb_optuna, 'xgb_v2.pkl')\n",
    "#joblib.dump(xgb_optuna_reg, 'xgb_v2_reg.pkl')\n",
    "#joblib.dump(vec_tuple[0], 'tfidf_vectorizer.pkl')\n",
    "#joblib.dump(vec_tuple[1], 'svd_transformer.pkl')\n",
    "\n",
    "#xgb_optuna.save_model(\"xgb_v2.json\") \n",
    "#xgb_optuna_reg.save_model(\"xgb_v2_reg_kfold.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c887d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16cb6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF SVD features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2999, 20), indices imply (2999, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m vec_tuple = (loaded_tfidf, loaded_svd)\n\u001b[32m      7\u001b[39m X_testing_raw = X_eval_raw.copy()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_testing_text, _ = \u001b[43mgenerate_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_testing_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_on_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvec_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m X_testing = featurize_table(X_testing_text)\n\u001b[32m     10\u001b[39m pred_testing = model_testing.predict(X_eval)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mgenerate_text_features\u001b[39m\u001b[34m(df, svd_components, is_train, fit_on_text)\u001b[39m\n\u001b[32m     50\u001b[39m     vectorizers = fit_on_text\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Create Columns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m svd_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvd_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msvd_desc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msvd_components\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# We reset index to make sure concat aligns correctly row-by-row\u001b[39;00m\n\u001b[32m     55\u001b[39m df_text = pd.concat([df_text.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m), svd_df], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:814\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    804\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    805\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    806\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m             copy=copy,\n\u001b[32m    812\u001b[39m         )\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:286\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    282\u001b[39m index, columns = _get_axes(\n\u001b[32m    283\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    284\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m values = values.T\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# if we don't have a dtype specified, then try to convert objects\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# on the entire block; this is to convert if we have datetimelike's\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# embedded in an object type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:359\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    357\u001b[39m passed = values.shape\n\u001b[32m    358\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (2999, 20), indices imply (2999, 5)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "# Testing if loaded models' working\n",
    "model_testing = joblib.load(\"../models/v2/xgb_v2.pkl\")\n",
    "loaded_tfidf = joblib.load(\"../models/v2/tfidf_vectorizer.pkl\")\n",
    "loaded_svd = joblib.load(\"../models/v2/svd_transformer.pkl\")\n",
    "vec_tuple = (loaded_tfidf, loaded_svd)\n",
    "X_testing_raw = X_eval_raw.copy()\n",
    "X_testing_text, _ = generate_text_features(X_testing_raw, is_train=False, fit_on_text=vec_tuple)\n",
    "X_testing = featurize_table(X_testing_text)\n",
    "pred_testing = model_testing.predict(X_eval)\n",
    "\n",
    "\n",
    "def evaluate_model(model, model_prediction):\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Kappa Score: {cohen_kappa_score(model_prediction, y_eval, weights='quadratic'):.4f}\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(model_prediction, y_eval):.4f}\")\n",
    "   \n",
    "\n",
    "print (evaluate_model(xgb_optuna, pred_xgb))\n",
    "print(\"\")\n",
    "print (evaluate_model(xgb_optuna_reg, pred_xgb_reg))\n",
    "#print(f'Optimal Quadratic kappa: {cohen_kappa_score(y_test, pred_reg_optimized, weights='quadratic'):.4f}')\n",
    "#print (\"Loaded QWK:\" , cohen_kappa_score(y_test, pred_testing, weights='quadratic'))\n",
    "\n",
    "#print (evaluate_model(clf, pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56851b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: which factors benefit the prediction the most (extracted from XGBoost)\n",
    "# ensure usage of original feature names not just np array indices\n",
    "\"\"\"\n",
    "xgb_optuna.get_booster().feature_names = list(X_train.columns)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "xgb.plot_importance(xgb_optuna, max_num_features=15, height=0.5, importance_type=\"gain\", values_format = \"{v:.2f}\") # Weight à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¸›à¸£à¸°à¸à¸­à¸š tree, Gain à¸­à¸°à¹„à¸£à¹à¸šà¹ˆà¸‡à¹„à¸”à¹‰à¸¡à¸²à¸à¸ªà¸¸à¸”\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Also plot for the regressor model\n",
    "xgb_optuna_reg.get_booster().feature_names = list(X_train.columns)\n",
    "plt.figure(figsize=(12,6))\n",
    "xgb.plot_importance(xgb_optuna_reg, max_num_features=15, height=0.5, importance_type=\"gain\", values_format=\"{v:.2f}\")\n",
    "plt.title(\"Feature Importance (Gain) - Regressor\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27da00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the classifier model\n",
    "xgb_optuna_reg.get_booster().feature_names = list(X_train.columns)\n",
    "importance_dict = xgb_optuna_reg.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# Sort by importance (ascending to get the least important features)\n",
    "sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "# Get bottom n features\n",
    "n = 10\n",
    "bottom_n = sorted_importance[:n]\n",
    "\n",
    "print(\"Bottom 5 least important features (by gain):\")\n",
    "for feature, importance in bottom_n:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Shit to drop:\", [f for f, _ in bottom_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c18da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
