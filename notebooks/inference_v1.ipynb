{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ae35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For handling module in diff dir\n",
    "import sys\n",
    "import os \n",
    "\n",
    "# Config\n",
    "TEST_DATA_PATH = '../data/test/test.csv'\n",
    "IMAGE_DIR = '../data/test_images/'\n",
    "OUTPUT_PATH = '../submission_v1.csv'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models/v1_stratify'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'train_images') \n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITIONS\n",
    "\n",
    "# 1. Text Model (Code from k4)\n",
    "class TransformerPetClassifier(nn.Module):\n",
    "    \"\"\"Transformer-based classifier for pet adoption speed prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=5, dropout=0.3):\n",
    "        super(TransformerPetClassifier, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Get hidden size from transformer config\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 2. Image Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)\n",
    "    def forward(self, x): return self.resnet(x)\n",
    "\n",
    "# 3. Tabular Model\n",
    "# no need to define xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85145e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tokenizer, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Image Processing\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['PetID']}-1.jpg\") \n",
    "        image = Image.new('RGB', (224, 224), (0, 0, 0)) \n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                pass \n",
    "        if self.transform: image = self.transform(image)\n",
    "\n",
    "        # 2. Text Processing\n",
    "        desc = str(row['Description']) if pd.notna(row['Description']) else \"no description\"\n",
    "        encoding = self.tokenizer(\n",
    "            desc, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe912386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_table(tabular_df):\n",
    "    # Namelength\n",
    "    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n",
    "    \n",
    "    # Description length\n",
    "    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    \n",
    "    \n",
    "    # Is Mixed Breed? (Breed2 is not 0)\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n",
    "    \n",
    "    # Number of Colors (Count non-zero color columns)\n",
    "    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1)\n",
    "    \n",
    "    # Is Free? (Fee is 0)\n",
    "    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)\n",
    "\n",
    "    # Fee per Pet (Normizalized for litters)\n",
    "    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0, 1)\n",
    "\n",
    "    # Total Media (Engagement proxy)\n",
    "    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt']\n",
    "\n",
    "    # Health Issue Flag (Health > 1 implies injury or condition)\n",
    "    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)\n",
    "    # --------------------\n",
    "    \n",
    "    # Encode state/breed as categories\n",
    "    # ADDED 'Type' to this list\n",
    "    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n",
    "                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n",
    "                    'Sterilized', 'Health', 'State']\n",
    "    for col in cat_cols:\n",
    "        if col in tabular_df.columns:\n",
    "            tabular_df[col] = tabular_df[col].astype('category')\n",
    "    return tabular_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff685371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_features(df, img_dir=IMG_DIR):\n",
    "    print(f\"Generating features for {len(df)} samples...\")\n",
    "    \n",
    "    # 1. XGBoost Inference\n",
    "    print(\"Loading XGBoost...\")\n",
    "    xgb_model = joblib.load(os.path.join(MODELS_DIR, 'xgb_stratify_optuna.pkl'))\n",
    "    \n",
    "    # Preprocess Tabular (Manual replication of src/tabular_model.py logic if not exposed as static method)\n",
    "    # Ideally: from src.tabular_model import TabularModel; TabularModel.preprocess(df)\n",
    "    \n",
    "    df_tab = featurize_table(df)\n",
    "    \n",
    "    drop_cols = ['Name', 'PetID', 'RescuerID', 'Description', 'AdoptionSpeed']\n",
    "    df_tab = df_tab.drop([c for c in drop_cols if c in df_tab.columns], axis=1)\n",
    "    \n",
    "    \n",
    "    xgb_probs = xgb_model.predict_proba(df_tab) # Shape (N, 5)\n",
    "\n",
    "    # 2. Load PyTorch Models\n",
    "    print(\"Loading DL Models...\")\n",
    "    # Image\n",
    "    img_model = ResNet().to(DEVICE)\n",
    "    img_state = torch.load(os.path.join(MODELS_DIR, 'pet_pred_resnet50.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in img_state: img_state = img_state['state_dict']\n",
    "    # FIX: Add 'resnet.' prefix to match the class definition\n",
    "    new_state_dict = {}\n",
    "    for k, v in img_state.items():\n",
    "        if not k.startswith('resnet.'):\n",
    "            new_state_dict['resnet.' + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    img_model.load_state_dict(new_state_dict)\n",
    "    img_model.eval()\n",
    "\n",
    "    # Text\n",
    "    text_model = TransformerPetClassifier(num_classes=5).to(DEVICE)\n",
    "    txt_state = torch.load(os.path.join(MODELS_DIR, 'best_transformer_model.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in txt_state: txt_state = txt_state['state_dict']\n",
    "    text_model.load_state_dict(txt_state, strict=False) \n",
    "    text_model.eval()\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dl = DataLoader(EnsembleDataset(df, img_dir, tokenizer, transform), batch_size=32, shuffle=False)\n",
    "\n",
    "    img_preds, text_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            input_ids, masks = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            img_out = img_model(imgs)\n",
    "            # FIX: Use softmax and extend with correct shape (N, 5) instead of flattening\n",
    "            img_preds.extend(torch.softmax(img_out, dim=1).cpu().numpy())\n",
    "            \n",
    "            text_out = text_model(input_ids, masks)\n",
    "            text_probs.extend(torch.softmax(text_out, dim=1).cpu().numpy())\n",
    "\n",
    "    # 4. Concatenate Features: XGB(5) + Text(5) + Image(5) = 15 Features\n",
    "    # FIX: Removed reshape(-1, 1) and flatten() to align dimensions\n",
    "    return np.hstack([xgb_probs, np.array(text_probs), np.array(img_preds)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e11ef",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Train...\n",
      "Generating features for 11994 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162906ddbd7f450495cf4b9284fd06e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 375/375 [07:52<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test...\n",
      "Generating features for 2999 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5d3f3675b84429a8a21d32b6b4d2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 94/94 [01:50<00:00,  1.18s/it]\n",
      "C:\\Users\\tanap\\AppData\\Local\\Temp\\ipykernel_32880\\4133397113.py:42: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:219.)\n",
      "  y_train_tensor = torch.LongTensor(y_train.values).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP Meta-Learner...\n",
      "Epoch 90 Loss: 1.1024\n",
      "Meta-Model (MLP) Test Kappa: 0.4163\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(os.path.join(DATA_DIR, 'train/train.csv'))\n",
    "\n",
    "# Create Splits\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    full_df, full_df['AdoptionSpeed'], test_size=0.2, random_state=42, stratify=full_df['AdoptionSpeed']  # Add stratify? , stratify=full_df['AdoptionSpeed']\n",
    ")\n",
    "# TODO: create a mapping for count encoding rescuerID -> for inference use the rescuer_counts to map \n",
    "rescuer_counts = train_df[\"RescuerID\"].value_counts()\n",
    "#rescuer_counts.to_csv('rescuer_counts.csv')\n",
    "train_df['rescuer_count'] = train_df['RescuerID'].map(rescuer_counts)\n",
    "\n",
    "test_df['rescuer_count'] = test_df['RescuerID'].map(rescuer_counts).fillna(0)\n",
    "\n",
    "train_df.drop('RescuerID', axis=1, inplace=True)\n",
    "test_df.drop('RescuerID', axis=1, inplace=True)\n",
    "# Generate Features (The \"Level 1\" Predictions)\n",
    "print(\"Generating Meta-Features for Train...\")\n",
    "X_train_meta = generate_ensemble_features(train_df) \n",
    "\n",
    "print(\"Generating Meta-Features for Test...\")\n",
    "X_test_meta = generate_ensemble_features(test_df)\n",
    "\n",
    "# - Use a Neural Network (Dense Layer) to combine & decide -\n",
    "class MetaModelMLP(nn.Module):\n",
    "    def __init__(self, input_dim=15, num_classes=5):\n",
    "        super(MetaModelMLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Prepare Data for PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train_meta).to(DEVICE)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(DEVICE)\n",
    "X_test_tensor = torch.FloatTensor(X_test_meta).to(DEVICE)\n",
    "\n",
    "# Training Loop\n",
    "meta_model = MetaModelMLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training MLP Meta-Learner...\")\n",
    "for epoch in range(100): # 100 epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = meta_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0: print(f\"Epoch {epoch} Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "# Evaluate\n",
    "meta_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = meta_model(X_test_tensor)\n",
    "    test_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"\\nMeta-Model (MLP) Test Kappa: {cohen_kappa_score(y_test, test_preds, weights='quadratic'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1821f25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fa90fa5b1ee11c86938398b60abc32cb</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa66486163b6cbc25ea62a34b11c9b91</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b53c34474d9e24574bcec6a3d3306a0d</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c00756f2bdd8fa88fc9f07a8309f7d5d</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ee2747ce26468ec44c7194e7d1d9dad9</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>e2af7d5c733a20fd2b1a273283986974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>089e417709c6a37839fc155af6b63196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>bc599c86ccd17d15a1c758b12d7e851b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>48d06353f65ac65dd35a8875b70962c5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>c026c581cda87aee6a1b629cfb54ef36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4789 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             RescuerID  count\n",
       "0     fa90fa5b1ee11c86938398b60abc32cb    377\n",
       "1     aa66486163b6cbc25ea62a34b11c9b91    262\n",
       "2     b53c34474d9e24574bcec6a3d3306a0d    180\n",
       "3     c00756f2bdd8fa88fc9f07a8309f7d5d    179\n",
       "4     ee2747ce26468ec44c7194e7d1d9dad9    123\n",
       "...                                ...    ...\n",
       "4784  e2af7d5c733a20fd2b1a273283986974      1\n",
       "4785  089e417709c6a37839fc155af6b63196      1\n",
       "4786  bc599c86ccd17d15a1c758b12d7e851b      1\n",
       "4787  48d06353f65ac65dd35a8875b70962c5      1\n",
       "4788  c026c581cda87aee6a1b629cfb54ef36      1\n",
       "\n",
       "[4789 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescuer_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8214958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test Data...\n",
      "Generating features for 3972 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c8181bae5478db0bf7e81d0a4465d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 125/125 [02:25<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to ../submission_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate prediction on test.csv!\n",
    "# Load test data\n",
    "inference_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Generate meta-features for test data\n",
    "print(\"Generating Meta-Features for Test Data...\")\n",
    "rescuer_counts = pd.read_csv(\"../data/experimental/rescuer_counts_mapping.csv\")\n",
    "inference_df['rescuer_count'] = inference_df['RescuerID'].map(rescuer_counts[\"RescuerID\"]).fillna(0)\n",
    "inference_df.drop(['RescuerID'],axis=1,  inplace=True)\n",
    "\n",
    "X_test_final_meta = generate_ensemble_features(inference_df, img_dir=IMAGE_DIR)\n",
    "\n",
    "# Convert to tensor\n",
    "X_test_final_tensor = torch.FloatTensor(X_test_final_meta).to(DEVICE)\n",
    "\n",
    "# Make predictions using trained meta model\n",
    "meta_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = meta_model(X_test_final_tensor)\n",
    "    final_test_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'PetID': inference_df['PetID'],\n",
    "    'AdoptionSpeed': final_test_preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(\"../submission_v1.csv\", index=False)\n",
    "print(f\"Submission saved to {\"../submission_v1.csv\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352a7920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m torch.save({\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mmodel\u001b[49m.state_dict(),\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m: MODEL_NAME,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m: NUM_CLASSES,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m: MAX_LENGTH,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_kappa\u001b[39m\u001b[33m'\u001b[39m: best_kappa\n\u001b[32m      7\u001b[39m }, \u001b[33m'\u001b[39m\u001b[33mmetamodel.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'best_kappa': best_kappa\n",
    "}, 'metamodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
