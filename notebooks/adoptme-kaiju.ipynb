{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10686,"databundleVersionId":861963,"sourceType":"competition"},{"sourceId":14870206,"sourceType":"datasetVersion","datasetId":9491791}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dependencies\nimport os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport xgboost as xgb\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.utils.class_weight import compute_sample_weight\n\n# For handling module in diff dir\nimport sys\nimport os \n\n# Config\nTEST_DATA_PATH = '/kaggle/input/petfinder-adoption-prediction/test/test.csv'\nIMAGE_DIR = '/kaggle/input/petfinder-adoption-prediction/test_images'\nOUTPUT_PATH = '../submission.csv'\nMODELS_DIR = \"/kaggle/input/datasets/thanaphonnaksri/testing/models/v1_stratify\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDATA_DIR = '/kaggle/input/petfinder-adoption-prediction'\nIMG_DIR = os.path.join(DATA_DIR, 'train_images') \n\nsys.path.append(os.path.abspath('..'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:29.339473Z","iopub.execute_input":"2026-02-17T20:01:29.339896Z","iopub.status.idle":"2026-02-17T20:01:35.021532Z","shell.execute_reply.started":"2026-02-17T20:01:29.339856Z","shell.execute_reply":"2026-02-17T20:01:35.020680Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Advanced Features\ndef extract_sentiment_from_json(pet_id, sentiment_dir=\"/kaggle/input/petfinder-adoption-prediction/train_sentiment\"):\n    # This assumes the sentiment files follow the pattern {PetID}.json\n    filename = f\"{sentiment_dir}/{pet_id}.json\"\n    try:\n        if os.path.exists(filename):\n            with open(filename, 'r') as f:\n                data = json.load(f)\n            # Usually 'documentSentiment' holds the overall score\n            if 'documentSentiment' in data:\n                return data['documentSentiment']['score'], data['documentSentiment']['magnitude']\n    except:\n        pass\n    return 0, 0 # Default if missing\n\n\ndef generate_text_features(df, svd_components=20, is_train=True, fit_on_text=None):\n    \"\"\"\n    df: The dataframe (containing 'Description' and 'PetID')\n    svd_components: Number of latent features to keep\n    is_train: Boolean, used to decide whether to fit or transform\n    fit_on_text: If is_train=False, pass the vectorizers here (tuple: tfidf, svd)\n    \"\"\"\n    df_text = df.copy()\n    \n    # 1. TF-IDF + SVD (Latent Semantic Analysis)\n    print(\"Generating TF-IDF SVD features...\")\n    descriptions = df_text['Description'].fillna(\"none\").astype(str)\n    \n    if is_train:\n        # Fit on TRAINING descriptions\n        tfidf = TfidfVectorizer(min_df=3,  max_features=1000, \n                                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n                                ngram_range=(1, 3), use_idf=True, smooth_idf=True, sublinear_tf=True,\n                                stop_words = 'english')\n        \n        svd = TruncatedSVD(n_components=svd_components, random_state=42)\n        \n        # Fit Transform\n        tf_vecs = tfidf.fit_transform(descriptions)\n        svd_vecs = svd.fit_transform(tf_vecs)\n        \n        # Save vectorizers for inference later\n        vectorizers = (tfidf, svd)\n    else:\n        # Load from passed tuple\n        tfidf, svd = fit_on_text\n        tf_vecs = tfidf.transform(descriptions)\n        svd_vecs = svd.transform(tf_vecs)\n        vectorizers = fit_on_text\n\n    # Create Columns\n    svd_df = pd.DataFrame(svd_vecs, columns=[f'svd_desc_{i}' for i in range(svd_components)])\n    svd_df.index = df_text.index\n    # We reset index to make sure concat aligns correctly row-by-row\n    df_text = pd.concat([df_text, svd_df], axis=1)\n\n    # 2. Sentiment Analysis (File-based lookup)\n    # Determine directory\n    sent_dir = \"/kaggle/input/petfinder-adoption-prediction/train_sentiment\" if is_train else \"/kaggle/input/petfinder-adoption-prediction/test_sentiment\"\n    \n    print(\"Extracting Sentiment...\")\n    # Apply row-wise (can be slow, maybe parallelize with pandarallel if needed)\n    sent_data = df_text['PetID'].apply(lambda x: extract_sentiment_from_json(x, sent_dir))\n    \n    df_text['sentiment_score'] = [x[0] for x in sent_data]\n    df_text['sentiment_magnitude'] = [x[1] for x in sent_data]\n    df_text['sentiment_polarity'] = df_text['sentiment_score'] * df_text['sentiment_magnitude']\n\n    return df_text, vectorizers\n\n# TODO: forward selection: use only features that improves kappa\ndef featurize_table(data_df):\n    tabular_df = data_df.copy()\n    # Namelength\n    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n    \n    # Description length\n    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n    \n    # Is Mixed Breed? (Breed2 is not 0)\n    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n    \n    \n    \n    # 1. Text\n    tabular_df['word_count'] = tabular_df['Description'].str.split().str.len().fillna(0)\n    tabular_df['char_count'] = tabular_df['Description'].str.len().fillna(0)\n    tabular_df['avg_word_len'] = tabular_df['char_count'] / (tabular_df['word_count'] + 1)\n    tabular_df['num_digits'] = tabular_df['Description'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n    tabular_df['all_caps_ratio'] = tabular_df['Description'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(1, len(str(x))))\n\n    # 2. Measures\n    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0,1)\n    tabular_df['photo_per_pet'] = tabular_df['PhotoAmt'] / tabular_df['Quantity']\n    tabular_df['age_per_size'] = tabular_df['Age'] / tabular_df['MaturitySize'] # Needs careful handling of 0s\n    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt'] # Total Media (Engagement proxy)\n    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1) # Number of Colors (Count non-zero color columns)\n    \n    \n    # 3. Simple Interactions\n    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0) & (tabular_df['Breed2'].notnull())\n    tabular_df['is_specific_color'] = (tabular_df['Color2'] != 0) # Has more than 1 color    \n    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)    # Is Free? (Fee is 0)\n    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)   # Health Issue Flag (Health > 1 implies injury or condition)\n    \n    # log transform for shit and giggles\n    tabular_df['Fee'] = np.log1p(tabular_df['Fee'])\n    tabular_df['PhotoAmt'] = np.log1p(tabular_df['PhotoAmt'])\n    \n    # Check whether this\n     \n    # Drop useless features -> does this actually works?\n    features_to_drop = [\"\"]\n    \n    # Encode categories\n    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n                    'Sterilized', 'Health', 'State']\n    \"\"\"\n    for col in cat_cols:\n        if col in tabular_df.columns:\n            tabular_df[col] = tabular_df[col].astype('category')\n    \"\"\"    \n\n    tabular_df.drop(['Name', 'PetID', 'Description'], axis=1, inplace=True)\n    return tabular_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.028775Z","iopub.execute_input":"2026-02-17T20:01:35.029048Z","iopub.status.idle":"2026-02-17T20:01:35.044761Z","shell.execute_reply.started":"2026-02-17T20:01:35.028999Z","shell.execute_reply":"2026-02-17T20:01:35.044055Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# MODEL DEFINITIONS\n\n# 1. Text Model (Code from k4)\nclass TransformerPetClassifier(nn.Module):\n    \"\"\"Transformer-based classifier for pet adoption speed prediction\"\"\"\n    # Fix this! make it local\n    def __init__(self, model_name='bert-base-uncased', num_classes=5, dropout=0.3):\n        super(TransformerPetClassifier, self).__init__()\n        \n        try:\n            self.transformer = AutoModel.from_pretrained(\"/kaggle/input/datasets/thanaphonnaksri/testing/models/bert_base\")\n        except OSError:\n            # Fallback: If only config exists (lighter upload), load config -> init model\n            from transformers import AutoConfig\n            config = AutoConfig.from_pretrained(\"/kaggle/input/datasets/thanaphonnaksri/testing/models/bert_config\")\n            self.transformer = AutoModel.from_config(config)\n            \n        self.dropout = nn.Dropout(dropout)\n        \n        # Get hidden size from transformer config\n        hidden_size = self.transformer.config.hidden_size\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask, return_features=False):\n        # Get transformer outputs\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        \n        if return_features:\n            # Pass through the first part of classification head (up to 128 dim)\n            x = self.classifier[0](pooled_output) # Linear 256\n            x = self.classifier[1](x) # ReLU\n            x = self.classifier[2](x) # Dropout\n            x = self.classifier[3](x) # Linear 128\n            return x # Return the 128-dim embedding\n        \n        return self.classifier(pooled_output)\n\n# 2. Image Model\nclass ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet, self).__init__()\n        self.resnet = models.resnet50(pretrained=False)\n        #self.resnet.load_state_dict(torch.load(\"/kaggle/input/datasets/thanaphonnaksri/testing/models/resnet50.pth\"))\n        self.fc = nn.Linear(self.resnet.fc.in_features, 5) # Save the Fully connected layer seperatedly for full prediction\n        self.resnet.fc = nn.Identity() # Identity: Remove the original head\n        \n    def forward(self, x, return_features=False): \n        features = self.resnet(x) # 2048-dim embedding\n        if return_features:\n            return features\n        return self.fc(features)\n\n# 3. Tabular Model\n# no need to define xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.048887Z","iopub.execute_input":"2026-02-17T20:01:35.049126Z","iopub.status.idle":"2026-02-17T20:01:35.060576Z","shell.execute_reply.started":"2026-02-17T20:01:35.049103Z","shell.execute_reply":"2026-02-17T20:01:35.059972Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# DATASET CLASS\nclass EnsembleDataset(Dataset):\n    def __init__(self, df, img_dir, tokenizer, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # 1. Image Processing\n        img_path = os.path.join(self.img_dir, f\"{row['PetID']}-1.jpg\") \n        image = Image.new('RGB', (224, 224), (0, 0, 0)) \n        if os.path.exists(img_path):\n            try:\n                image = Image.open(img_path).convert('RGB')\n            except:\n                pass \n        if self.transform: image = self.transform(image)\n\n        # 2. Text Processing\n        desc = str(row['Description']) if pd.notna(row['Description']) else \"no description\"\n        encoding = self.tokenizer(\n            desc, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n        )\n\n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.061613Z","iopub.execute_input":"2026-02-17T20:01:35.061927Z","iopub.status.idle":"2026-02-17T20:01:35.074834Z","shell.execute_reply.started":"2026-02-17T20:01:35.061880Z","shell.execute_reply":"2026-02-17T20:01:35.073980Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# TODO: forward selection: use only features that improves kappa\ndef featurize_table(data_df):\n    tabular_df = data_df.copy()\n    # Namelength\n    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n    \n    # Description length\n    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n    \n    # Is Mixed Breed? (Breed2 is not 0)\n    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n    \n    \n    \n    # 1. Text\n    tabular_df['word_count'] = tabular_df['Description'].str.split().str.len().fillna(0)\n    tabular_df['char_count'] = tabular_df['Description'].str.len().fillna(0)\n    tabular_df['avg_word_len'] = tabular_df['char_count'] / (tabular_df['word_count'] + 1)\n    tabular_df['num_digits'] = tabular_df['Description'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n    tabular_df['all_caps_ratio'] = tabular_df['Description'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(1, len(str(x))))\n\n    # 2. Measures\n    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0,1)\n    tabular_df['photo_per_pet'] = tabular_df['PhotoAmt'] / tabular_df['Quantity']\n    tabular_df['age_per_size'] = tabular_df['Age'] / tabular_df['MaturitySize'] # Needs careful handling of 0s\n    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt'] # Total Media (Engagement proxy)\n    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1) # Number of Colors (Count non-zero color columns)\n    \n    \n    # 3. Simple Interactions\n    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0) & (tabular_df['Breed2'].notnull())\n    tabular_df['is_specific_color'] = (tabular_df['Color2'] != 0) # Has more than 1 color    \n    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)    # Is Free? (Fee is 0)\n    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)   # Health Issue Flag (Health > 1 implies injury or condition)\n    \n    # log transform for shit and giggles\n    tabular_df['Fee'] = np.log1p(tabular_df['Fee'])\n    tabular_df['PhotoAmt'] = np.log1p(tabular_df['PhotoAmt'])\n    \n    # Check whether this\n     \n    # Drop useless features -> does this actually works?\n    features_to_drop = [\"\"]\n    \n    # Encode categories\n    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n                    'Sterilized', 'Health', 'State']\n      \n\n    tabular_df.drop(['Name', 'PetID', 'Description', \"RescuerID\"], axis=1, inplace=True)\n    return tabular_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.075830Z","iopub.execute_input":"2026-02-17T20:01:35.076210Z","iopub.status.idle":"2026-02-17T20:01:35.086882Z","shell.execute_reply.started":"2026-02-17T20:01:35.076185Z","shell.execute_reply":"2026-02-17T20:01:35.086080Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Advanced Features\ndef extract_sentiment_from_json(pet_id, sentiment_dir=\"../data/train_sentiment/\"):\n    # This assumes the sentiment files follow the pattern {PetID}.json\n    filename = f\"{sentiment_dir}/{pet_id}.json\"\n    try:\n        if os.path.exists(filename):\n            with open(filename, 'r') as f:\n                data = json.load(f)\n            # Usually 'documentSentiment' holds the overall score\n            if 'documentSentiment' in data:\n                return data['documentSentiment']['score'], data['documentSentiment']['magnitude']\n    except:\n        pass\n    return 0, 0 # Default if missing\n\n\ndef generate_text_features(df, svd_components=20, is_train=True, fit_on_text=None):\n    \"\"\"\n    df: The dataframe (containing 'Description' and 'PetID')\n    svd_components: Number of latent features to keep\n    is_train: Boolean, used to decide whether to fit or transform\n    fit_on_text: If is_train=False, pass the vectorizers here (tuple: tfidf, svd)\n    \"\"\"\n    df_text = df.copy()\n    \n    # 1. TF-IDF + SVD (Latent Semantic Analysis)\n    print(\"Generating TF-IDF SVD features...\")\n    descriptions = df_text['Description'].fillna(\"none\").astype(str)\n    \n    if is_train:\n        # Fit on TRAINING descriptions\n        tfidf = TfidfVectorizer(min_df=3,  max_features=1000, \n                                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n                                ngram_range=(1, 3), use_idf=True, smooth_idf=True, sublinear_tf=True,\n                                stop_words = 'english')\n        \n        svd = TruncatedSVD(n_components=svd_components, random_state=42)\n        \n        # Fit Transform\n        tf_vecs = tfidf.fit_transform(descriptions)\n        svd_vecs = svd.fit_transform(tf_vecs)\n        \n        # Save vectorizers for inference later\n        vectorizers = (tfidf, svd)\n    else:\n        # Load from passed tuple\n        tfidf, svd = fit_on_text\n        tf_vecs = tfidf.transform(descriptions)\n        svd_vecs = svd.transform(tf_vecs)\n        vectorizers = fit_on_text\n\n    # Create Columns\n    svd_df = pd.DataFrame(svd_vecs, columns=[f'svd_desc_{i}' for i in range(svd_components)])\n    # We reset index to make sure concat aligns correctly row-by-row\n    df_text = pd.concat([df_text.reset_index(drop=True), svd_df], axis=1)\n\n    # 2. Sentiment Analysis (File-based lookup)\n    # Determine directory\n    sent_dir = \"../data/train_sentiment\" if is_train else \"../data/test_sentiment\"\n    \n    print(\"Extracting Sentiment...\")\n    # Apply row-wise (can be slow, maybe parallelize with pandarallel if needed)\n    sent_data = df_text['PetID'].apply(lambda x: extract_sentiment_from_json(x, sent_dir))\n    \n    df_text['sentiment_score'] = [x[0] for x in sent_data]\n    df_text['sentiment_magnitude'] = [x[1] for x in sent_data]\n    df_text['sentiment_polarity'] = df_text['sentiment_score'] * df_text['sentiment_magnitude']\n\n    return df_text, vectorizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.088064Z","iopub.execute_input":"2026-02-17T20:01:35.088463Z","iopub.status.idle":"2026-02-17T20:01:35.100770Z","shell.execute_reply.started":"2026-02-17T20:01:35.088429Z","shell.execute_reply":"2026-02-17T20:01:35.100073Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def generate_ensemble_features(df, img_dir=IMG_DIR):\n    print(f\"Generating features for {len(df)} samples...\")\n    \n    # 1. XGBoost Inference\n    print(\"Loading XGBoost...\")\n    xgb_model = xgb.XGBRegressor()\n    xgb_model.load_model(\"/kaggle/input/datasets/thanaphonnaksri/testing/models/v2/xgb_v2_reg_kfold.json\")\n    \n\n    df_tab = featurize_table(df)\n    \n    drop_cols = ['Name', 'PetID', 'RescuerID', 'Description', 'AdoptionSpeed']\n    df_tab = df_tab.drop([c for c in drop_cols if c in df_tab.columns], axis=1)\n\n    xgb_preds = xgb_model.predict(df_tab)\n    \n    # 2. Load PyTorch Models\n    print(\"Loading DL Models...\")\n    # Image\n    img_model = ResNet().to(DEVICE)\n    img_state = torch.load(os.path.join(MODELS_DIR, 'pet_pred_resnet50.pth'), map_location=DEVICE)\n    if 'state_dict' in img_state: img_state = img_state['state_dict']\n    # FIX: Add 'resnet.' prefix to match the class definition\n    new_state_dict = {}\n    for k, v in img_state.items():\n        \n        # Case 1: the key is for FC layer\n        if \"resnet.fc.\" in k:\n            new_key = k.replace(\"resnet.fc.\", \"fc.\")\n            new_state_dict[new_key] = v\n            continue\n        # Case 2: the key is for the backbone\n        if not k.startswith('resnet.') and 'fc.' not in k:\n            new_state_dict['resnet.' + k] = v\n        else:\n            # It already matches\n            new_state_dict[k] = v\n    \n    img_model.load_state_dict(new_state_dict)\n    img_model.eval()\n\n    # Text\n    text_model = TransformerPetClassifier(num_classes=5).to(DEVICE)\n    txt_state = torch.load(os.path.join(MODELS_DIR, 'best_transformer_model.pth'), map_location=DEVICE)\n    if 'state_dict' in txt_state: txt_state = txt_state['state_dict']\n    text_model.load_state_dict(txt_state, strict=False) \n    text_model.eval()\n\n    # 3. Inference Loop\n    # CHANGE: Point to the local directory containing tokenizer_config.json\n    tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/datasets/thanaphonnaksri/testing/models/tokenizer\")\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    dl = DataLoader(EnsembleDataset(df, img_dir, tokenizer, transform), batch_size=32, shuffle=False)\n    \n    img_features, text_features = [], []\n\n    img_preds_list, text_preds_list = [], []\n    \n    # Softmax for converting logits to probs\n    softmax = nn.Softmax(dim=1)\n    with torch.no_grad():\n        for batch in tqdm(dl):\n            imgs = batch['image'].to(DEVICE)\n            input_ids, masks = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n\n            # --- IMAGE PREDICTION ---\n            img_logits = img_model(imgs) # Output: [Batch, 5]\n            img_probs = softmax(img_logits).cpu().numpy()\n            # Convert to scalar: sum(prob * class_idx)\n            img_scores = np.sum(img_probs * np.arange(5), axis=1)\n            img_preds_list.extend(img_scores)\n            \n            # --- TEXT PREDICTION ---\n            text_logits = text_model(input_ids, masks) # Output: [Batch, 5]\n            text_probs = softmax(text_logits).cpu().numpy()\n            text_scores = np.sum(text_probs * np.arange(5), axis=1)\n            text_preds_list.extend(text_scores)\n            \"\"\"\n            # Changed extrect emb instead of probs\n            # Image: Returns (Batch, 2048)\n            img_emb = img_model(imgs, return_features=True) \n            # Flatten 4D tensor (N, 2048, 1, 1) -> (N, 2048)\n            img_emb = img_emb.view(img_emb.size(0), -1)\n            img_features.extend(img_emb.cpu().numpy())\n            \n            # Text: Returns (Batch, 128)\n            text_emb = text_model(input_ids, masks, return_features=True)\n            text_features.extend(text_emb.cpu().numpy())\n            \n    # 4. Concatenate Features: XGB(5) + Text(128) + Image(2048)\n    # FIX: Removed reshape(-1, 1) and flatten() to align dimensions\n    return np.hstack([xgb_probs, np.array(text_features), np.array(img_features)])\n            \"\"\"\n    return xgb_preds, np.array(img_preds_list), np.array(text_preds_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.101813Z","iopub.execute_input":"2026-02-17T20:01:35.102119Z","iopub.status.idle":"2026-02-17T20:01:35.115198Z","shell.execute_reply.started":"2026-02-17T20:01:35.102096Z","shell.execute_reply":"2026-02-17T20:01:35.114334Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Regressor instead?\nclass IntermediateFusionMetaModel(nn.Module):\n    def __init__(self, xgb_dim=5, text_dim=128, img_dim=2048):\n        super(IntermediateFusionMetaModel, self).__init__()\n        \n        # IDK: Normalize inputs for scaling\n        self.norm_xgb = nn.BatchNorm1d(xgb_dim)\n        self.norm_text = nn.BatchNorm1d(text_dim)\n        self.norm_img = nn.BatchNorm1d(img_dim)\n        \n        # Projectors to reduce dimensionality before fusion\n        self.img_projector = nn.Sequential(\n            nn.Linear(img_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.text_projector = nn.Sequential(\n            nn.Linear(text_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU()\n        )\n        \n        # Fusion Layer\n        self.fusion = nn.Sequential(\n            nn.Linear(256 + 64 + xgb_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64,1) #1 for single continuous value (reg)\n            \n        )\n        \n    def forward(self, x):\n        # Slice the input back into components\n        # x is [xgb(5), text(128), img(2048)]\n        xgb_start, xgb_end = 0, 5\n        text_start, text_end = 5, 5+128\n        img_start = 5+128\n        \n        xgb_data = x[:, xgb_start:xgb_end]\n        text_data = x[:, text_start:text_end]\n        img_data = x[:, img_start:]\n        \n        # IDK Apply Normalization\n        xgb_data = self.norm_xgb(xgb_data)\n        text_data = self.norm_text(text_data)\n        img_data = self.norm_img(img_data)\n        \n        # Project\n        img_emb = self.img_projector(img_data)\n        text_emb = self.text_projector(text_data)\n        \n        # Concatenate and Classify\n        combined = torch.cat([xgb_data, text_emb, img_emb], dim=1)\n        return self.fusion(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.116210Z","iopub.execute_input":"2026-02-17T20:01:35.116774Z","iopub.status.idle":"2026-02-17T20:01:35.130456Z","shell.execute_reply.started":"2026-02-17T20:01:35.116746Z","shell.execute_reply":"2026-02-17T20:01:35.129901Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import scipy as sp\nfrom functools import partial\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') # Optimizer\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p.astype(int)\n\n    def coefficients(self):\n        return self.coef_['x']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.131624Z","iopub.execute_input":"2026-02-17T20:01:35.131885Z","iopub.status.idle":"2026-02-17T20:01:35.144774Z","shell.execute_reply.started":"2026-02-17T20:01:35.131853Z","shell.execute_reply":"2026-02-17T20:01:35.144042Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Main Execution","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfull_df = pd.read_csv(os.path.join(DATA_DIR, 'train/train.csv'))\n\nX_train, X_eval, y_train, y_test = train_test_split(\n    full_df, full_df['AdoptionSpeed'], test_size=0.2, random_state=42, stratify=full_df['AdoptionSpeed']\n)\n\nprint(\"Generating Train Features...\")\ntrain_text, vec_tuple = generate_text_features(X_train, is_train=True)\nX_train_meta = generate_ensemble_features(train_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:01:35.145784Z","iopub.execute_input":"2026-02-17T20:01:35.146188Z","iopub.status.idle":"2026-02-17T20:03:26.329805Z","shell.execute_reply.started":"2026-02-17T20:01:35.146152Z","shell.execute_reply":"2026-02-17T20:03:26.328959Z"}},"outputs":[{"name":"stdout","text":"Generating Train Features...\nGenerating TF-IDF SVD features...\nExtracting Sentiment...\nGenerating features for 11994 samples...\nLoading XGBoost...\nLoading DL Models...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n2026-02-17 20:01:39.759000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771358499.776142    1069 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771358499.781149    1069 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771358499.794808    1069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771358499.794832    1069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771358499.794835    1069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771358499.794837    1069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n100%|██████████| 375/375 [01:42<00:00,  3.64it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def shaping(meta_feature):\n    if isinstance(meta_feature, tuple):\n        p1, p2, p3 = meta_feature\n        \n        # Ensure all are 2D\n        if p1.ndim == 1: p1 = p1.reshape(-1, 1)\n        if p2.ndim == 1: p2 = p2.reshape(-1, 1)\n        if p3.ndim == 1: p3 = p3.reshape(-1, 1)\n        \n        meta_feature = np.hstack([p1, p2, p3])\n        print(\"New shape:\", meta_feature.shape)\n        return meta_feature\nX_train_meta = shaping(X_train_meta)\nprint(\"Fitting Meta Model...\")\nmeta_model = LogisticRegression(C=1.0) # You can tune C\nmeta_model.fit(X_train_meta, y_train)\n\nprint(\"Optimizing Rounder Thresholds...\")\n# Need val features\nval_text, _ = generate_text_features(X_eval, is_train=False, fit_on_text=vec_tuple)\nX_val_meta = generate_ensemble_features(val_text)\nX_val_meta = shaping(X_val_meta)\nval_probs = meta_model.predict_proba(X_val_meta)\nval_scores = np.sum(val_probs * np.arange(5), axis=1) # Expected Value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:03:26.332420Z","iopub.execute_input":"2026-02-17T20:03:26.333032Z","iopub.status.idle":"2026-02-17T20:03:54.322773Z","shell.execute_reply.started":"2026-02-17T20:03:26.332987Z","shell.execute_reply":"2026-02-17T20:03:54.321937Z"}},"outputs":[{"name":"stdout","text":"New shape: (11994, 3)\nFitting Meta Model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Optimizing Rounder Thresholds...\nGenerating TF-IDF SVD features...\nExtracting Sentiment...\nGenerating features for 2999 samples...\nLoading XGBoost...\nLoading DL Models...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n100%|██████████| 94/94 [00:26<00:00,  3.59it/s]","output_type":"stream"},{"name":"stdout","text":"New shape: (2999, 3)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(val_scores, y_test.values)\ncoefficients = optR.coefficients()\nprint(f\"Optimized Coefficients: {coefficients}\")\n\n# Check Val Score\nval_preds = optR.predict(val_scores, coefficients)\nprint(f\"Optimized Validation Kappa: {cohen_kappa_score(y_test, val_preds, weights='quadratic'):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:03:54.323750Z","iopub.execute_input":"2026-02-17T20:03:54.323981Z","iopub.status.idle":"2026-02-17T20:03:54.705486Z","shell.execute_reply.started":"2026-02-17T20:03:54.323956Z","shell.execute_reply":"2026-02-17T20:03:54.704692Z"}},"outputs":[{"name":"stdout","text":"Optimized Coefficients: [0.52120002 1.58311194 2.59319347 3.12369591]\nOptimized Validation Kappa: 0.4621\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- INFERENCE PHASE ---\nprint(\"Starting Inference on Test Data...\")\ninference_df = pd.read_csv(TEST_DATA_PATH)\n\n# 1. Feature Gen (Test)\ntest_final_text, _ = generate_text_features(inference_df, is_train=False, fit_on_text=vec_tuple)\nX_test_final_meta = generate_ensemble_features(test_final_text, img_dir=IMAGE_DIR)\nX_test_final_meta = shaping(X_test_final_meta)\n# 2. Predict Probabilities -> Regression Score\ntest_probs = meta_model.predict_proba(X_test_final_meta)\ntest_scores = np.sum(test_probs * np.arange(5), axis=1)\n\n# 3. Apply Thresholds\nfinal_test_preds = optR.predict(test_scores, coefficients)\n\n# 4. Save\nsubmission_df = pd.DataFrame({\n    'PetID': inference_df['PetID'],\n    'AdoptionSpeed': final_test_preds\n})\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T20:03:54.706530Z","iopub.execute_input":"2026-02-17T20:03:54.706837Z","iopub.status.idle":"2026-02-17T20:04:30.174223Z","shell.execute_reply.started":"2026-02-17T20:03:54.706813Z","shell.execute_reply":"2026-02-17T20:04:30.173409Z"}},"outputs":[{"name":"stdout","text":"Starting Inference on Test Data...\nGenerating TF-IDF SVD features...\nExtracting Sentiment...\nGenerating features for 3972 samples...\nLoading XGBoost...\nLoading DL Models...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n100%|██████████| 125/125 [00:33<00:00,  3.68it/s]","output_type":"stream"},{"name":"stdout","text":"New shape: (3972, 3)\nSubmission saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13}]}