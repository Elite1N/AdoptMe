{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\"\"\"\n",
    "1. load the models from checkpoint\n",
    "2. make prediction on train.csv -> create probs for each label \n",
    "3. use probs to make 15 column th\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ae35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "TABULAR_MODEL_PATH = '../models/xgboost_tabular.joblib'\n",
    "IMAGE_MODEL_PATH = '../models/resnet_image.pth'\n",
    "TEXT_MODEL_PATH = '../models/distilbert_text.pth'\n",
    "TEST_DATA_PATH = '../data/test/test.csv'\n",
    "IMAGE_DIR = '../data/test_images/'\n",
    "OUTPUT_PATH = '../submission.csv'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models/v1'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'train_images') \n",
    "\n",
    "# Dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "# For handling module in diff dir\n",
    "import sys\n",
    "import os \n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITIONS\n",
    "\n",
    "# 1. Text Model (Code from k4)\n",
    "class TransformerPetClassifier(nn.Module):\n",
    "    \"\"\"Transformer-based classifier for pet adoption speed prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=5, dropout=0.3):\n",
    "        super(TransformerPetClassifier, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Get hidden size from transformer config\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 2. Image Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)\n",
    "    def forward(self, x): return self.resnet(x)\n",
    "\n",
    "# 3. Tabular Model\n",
    "# no need to define xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85145e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tokenizer, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Image Processing\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['PetID']}-1.jpg\") \n",
    "        image = Image.new('RGB', (224, 224), (0, 0, 0)) \n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                pass \n",
    "        if self.transform: image = self.transform(image)\n",
    "\n",
    "        # 2. Text Processing\n",
    "        desc = str(row['Description']) if pd.notna(row['Description']) else \"no description\"\n",
    "        encoding = self.tokenizer(\n",
    "            desc, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'original_row': row\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff685371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_features(df, img_dir=IMG_DIR):\n",
    "    print(f\"Generating features for {len(df)} samples...\")\n",
    "    \n",
    "    # 1. XGBoost Inference\n",
    "    print(\"Loading XGBoost...\")\n",
    "    xgb_model = joblib.load(os.path.join(MODELS_DIR, 'xgb_best_model.pkl'))\n",
    "    \n",
    "    # Preprocess Tabular (Manual replication of src/tabular_model.py logic if not exposed as static method)\n",
    "    # Ideally: from src.tabular_model import TabularModel; TabularModel.preprocess(df)\n",
    "    \n",
    "    df_tab = df.copy()\n",
    "    if 'Name' in df_tab.columns: df_tab[\"name_length\"] = df_tab['Name'].str.len().fillna(0)\n",
    "    if 'Description' in df_tab.columns: df_tab['description_length'] = df_tab['Description'].str.len().fillna(0)\n",
    "    drop_cols = ['Name', 'PetID', 'RescuerID', 'Description', 'AdoptionSpeed']\n",
    "    df_tab = df_tab.drop([c for c in drop_cols if c in df_tab.columns], axis=1)\n",
    "    \n",
    "    \n",
    "    xgb_probs = xgb_model.predict_proba(df_tab) # Shape (N, 5)\n",
    "\n",
    "    # 2. Load PyTorch Models\n",
    "    print(\"Loading DL Models...\")\n",
    "    # Image\n",
    "    img_model = ResNet().to(DEVICE)\n",
    "    img_state = torch.load(os.path.join(MODELS_DIR, 'pet_pred_resnet50.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in img_state: img_state = img_state['state_dict']\n",
    "    # FIX: Add 'resnet.' prefix to match the class definition\n",
    "    new_state_dict = {}\n",
    "    for k, v in img_state.items():\n",
    "        if not k.startswith('resnet.'):\n",
    "            new_state_dict['resnet.' + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    img_model.load_state_dict(new_state_dict)\n",
    "    img_model.eval()\n",
    "\n",
    "    # Text\n",
    "    text_model = TransformerPetClassifier(num_classes=5).to(DEVICE)\n",
    "    txt_state = torch.load(os.path.join(MODELS_DIR, 'best_transformer_model.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in txt_state: txt_state = txt_state['state_dict']\n",
    "    text_model.load_state_dict(txt_state, strict=False) \n",
    "    text_model.eval()\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dl = DataLoader(EnsembleDataset(df, img_dir, tokenizer, transform), batch_size=32, shuffle=False)\n",
    "\n",
    "    img_preds, text_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            input_ids, masks = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            img_out = img_model(imgs)\n",
    "            img_preds.extend(img_out.cpu().numpy().flatten())\n",
    "            \n",
    "            text_out = text_model(input_ids, masks)\n",
    "            text_probs.extend(torch.softmax(text_out, dim=1).cpu().numpy())\n",
    "\n",
    "    # 4. Concatenate Features: XGB(5) + Text(5) + Image(1) = 11 Features\n",
    "    return np.hstack([xgb_probs, np.array(text_probs), np.array(img_preds).reshape(-1, 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e11ef",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea0f2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for 11994 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89464a746e2e475ea05da7d6a20b45d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "  0%|          | 0/375 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    169\u001b[39m clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m clone.update(\n\u001b[32m    171\u001b[39m     {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m     }\n\u001b[32m    177\u001b[39m )\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:243\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    238\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    239\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    240\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    241\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.Series'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m train_df, test_df, y_train, y_test = train_test_split(\n\u001b[32m      5\u001b[39m     full_df, full_df[\u001b[33m'\u001b[39m\u001b[33mAdoptionSpeed\u001b[39m\u001b[33m'\u001b[39m], test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=full_df[\u001b[33m'\u001b[39m\u001b[33mAdoptionSpeed\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Generate Features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_train_meta = \u001b[43mgenerate_ensemble_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Features for Meta-Training\u001b[39;00m\n\u001b[32m     10\u001b[39m X_test_meta = generate_ensemble_features(test_df)   \u001b[38;5;66;03m# Features for Final Evaluation\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Train Meta-Model (Logistic Regression)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mgenerate_ensemble_features\u001b[39m\u001b[34m(df, img_dir)\u001b[39m\n\u001b[32m     54\u001b[39m img_preds, text_probs = [], []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:57\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:401\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    181\u001b[39m                 {\n\u001b[32m    182\u001b[39m                     key: collate(\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m                 }\n\u001b[32m    187\u001b[39m             )\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[32m    190\u001b[39m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    194\u001b[39m         }\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[32m    197\u001b[39m         *(\n\u001b[32m    198\u001b[39m             collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*batch, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:243\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    236\u001b[39m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[32m    237\u001b[39m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[32m    238\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    239\u001b[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001b[32m    240\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    241\u001b[39m             ]\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.Series'>"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(os.path.join(DATA_DIR, 'train/train.csv'))\n",
    "\n",
    "# Create Splits\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    full_df, full_df['AdoptionSpeed'], test_size=0.2, random_state=42, stratify=full_df['AdoptionSpeed']\n",
    ")\n",
    "\n",
    "# Generate Features\n",
    "X_train_meta = generate_ensemble_features(train_df) # Features for Meta-Training\n",
    "X_test_meta = generate_ensemble_features(test_df)   # Features for Final Evaluation\n",
    "\n",
    "# Train Meta-Model (Logistic Regression)\n",
    "meta_model = LogisticRegression(solver='liblinear')\n",
    "meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "# Evaluate\n",
    "test_preds = meta_model.predict(X_test_meta)\n",
    "print(f\"Meta-Model Test Kappa: {cohen_kappa_score(y_test, test_preds, weights='quadratic'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07aedff",
   "metadata": {},
   "source": [
    "# RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalWithXGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalWithXGB, self).__init__()\n",
    "\n",
    "        # --- Model 1 Input (ผลลัพธ์จาก XGBoost 5 ตัว) ---\n",
    "        self.tab_path = nn.Sequential(\n",
    "            nn.Linear(5, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- Model 2 & 3 (NLP & Image - เหมือนเดิม) ---\n",
    "        # สมมติผลลัพธ์จาก Image/Text ได้ฝั่งละ 32 features\n",
    "        self.nlp_mock = nn.Sequential(nn.Linear(768, 32), nn.ReLU())\n",
    "        self.img_mock = nn.Sequential(nn.Linear(1024, 32), nn.ReLU())\n",
    "\n",
    "        # --- Combination Layer (กล่องม่วง) ---\n",
    "        self.combined_dense = nn.Sequential(\n",
    "            nn.Linear(16 + 32 + 32, 64), # 16(จาก XGB) + 32(Text) + 32(Image)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) # ทำนาย AdoptionSpeed\n",
    "        )\n",
    "\n",
    "    def forward(self, xgb_probs, text_features, img_features):\n",
    "        x_tab = self.tab_path(xgb_probs)\n",
    "        x_nlp = self.nlp_mock(text_features)\n",
    "        x_img = self.img_mock(img_features)\n",
    "\n",
    "        combined = torch.cat([x_tab, x_nlp, x_img], dim=1)\n",
    "        return self.combined_dense(combined)\n",
    "\n",
    "model = MultiModalWithXGB().to(device)\n",
    "print(\"Model Ready with XGBoost integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.xgb_prob_cols = [f'xgb_prob_{i}' for i in range(5)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Explicitly cast to float32 to prevent TypeError\n",
    "        xgb_probs = torch.tensor(row[self.xgb_prob_cols].values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        # Mock NLP and Image features as in the original model definition\n",
    "        text_features = torch.zeros(768, dtype=torch.float32)  # Mock NLP features\n",
    "        img_features = torch.zeros(1024, dtype=torch.float32)  # Mock Image features\n",
    "\n",
    "        # Convert target to float32\n",
    "        target = torch.tensor(row['AdoptionSpeed'], dtype=torch.float32)\n",
    "\n",
    "        return xgb_probs, text_features, img_features, target\n",
    "\n",
    "# เตรียม DataLoader\n",
    "train_set, val_set = train_test_split(train_df, test_size=0.2)\n",
    "train_loader = DataLoader(PetDataset(train_set), batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = nn.SmoothL1Loss() # ตามแผนผัง\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# วนลูปเทรนสั้นๆ เป็นตัวอย่าง\n",
    "model.train()\n",
    "for epoch in range(10): # ลอง 1 epoch ก่อน\n",
    "    for xgb_probs, text_features, img_features, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xgb_probs.to(device), text_features.to(device), img_features.to(device))\n",
    "        loss = criterion(output.squeeze(), target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "# Save Checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'petfinder_full_model.pth')\n",
    "\n",
    "print(\"\\nModel Trained and Checkpoint Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
