{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\"\"\"\n",
    "1. load the models from checkpoint\n",
    "2. make prediction on train.csv -> create probs for each label \n",
    "3. use probs to make 15 column th\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41ae35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "TABULAR_MODEL_PATH = '../models/xgboost_tabular.joblib'\n",
    "IMAGE_MODEL_PATH = '../models/resnet_image.pth'\n",
    "TEXT_MODEL_PATH = '../models/distilbert_text.pth'\n",
    "TEST_DATA_PATH = '../data/test/test.csv'\n",
    "IMAGE_DIR = '../data/test_images/'\n",
    "OUTPUT_PATH = '../submission.csv'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models/v1'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'train_images') \n",
    "\n",
    "# Dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For handling module in diff dir\n",
    "import sys\n",
    "import os \n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITIONS\n",
    "\n",
    "# 1. Text Model (Code from k4)\n",
    "class TransformerPetClassifier(nn.Module):\n",
    "    \"\"\"Transformer-based classifier for pet adoption speed prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=5, dropout=0.3):\n",
    "        super(TransformerPetClassifier, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Get hidden size from transformer config\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 2. Image Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)\n",
    "    def forward(self, x): return self.resnet(x)\n",
    "\n",
    "# 3. Tabular Model\n",
    "# no need to define xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e85145e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tokenizer, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Image Processing\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['PetID']}-1.jpg\") \n",
    "        image = Image.new('RGB', (224, 224), (0, 0, 0)) \n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                pass \n",
    "        if self.transform: image = self.transform(image)\n",
    "\n",
    "        # 2. Text Processing\n",
    "        desc = str(row['Description']) if pd.notna(row['Description']) else \"no description\"\n",
    "        encoding = self.tokenizer(\n",
    "            desc, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff685371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_features(df, img_dir=IMG_DIR):\n",
    "    print(f\"Generating features for {len(df)} samples...\")\n",
    "    \n",
    "    # 1. XGBoost Inference\n",
    "    print(\"Loading XGBoost...\")\n",
    "    xgb_model = joblib.load(os.path.join(MODELS_DIR, 'xgb_best_model.pkl'))\n",
    "    \n",
    "    # Preprocess Tabular (Manual replication of src/tabular_model.py logic if not exposed as static method)\n",
    "    # Ideally: from src.tabular_model import TabularModel; TabularModel.preprocess(df)\n",
    "    \n",
    "    df_tab = df.copy()\n",
    "    if 'Name' in df_tab.columns: df_tab[\"name_length\"] = df_tab['Name'].str.len().fillna(0)\n",
    "    if 'Description' in df_tab.columns: df_tab['description_length'] = df_tab['Description'].str.len().fillna(0)\n",
    "    drop_cols = ['Name', 'PetID', 'RescuerID', 'Description', 'AdoptionSpeed']\n",
    "    df_tab = df_tab.drop([c for c in drop_cols if c in df_tab.columns], axis=1)\n",
    "    \n",
    "    \n",
    "    xgb_probs = xgb_model.predict_proba(df_tab) # Shape (N, 5)\n",
    "\n",
    "    # 2. Load PyTorch Models\n",
    "    print(\"Loading DL Models...\")\n",
    "    # Image\n",
    "    img_model = ResNet().to(DEVICE)\n",
    "    img_state = torch.load(os.path.join(MODELS_DIR, 'pet_pred_resnet50.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in img_state: img_state = img_state['state_dict']\n",
    "    # FIX: Add 'resnet.' prefix to match the class definition\n",
    "    new_state_dict = {}\n",
    "    for k, v in img_state.items():\n",
    "        if not k.startswith('resnet.'):\n",
    "            new_state_dict['resnet.' + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    img_model.load_state_dict(new_state_dict)\n",
    "    img_model.eval()\n",
    "\n",
    "    # Text\n",
    "    text_model = TransformerPetClassifier(num_classes=5).to(DEVICE)\n",
    "    txt_state = torch.load(os.path.join(MODELS_DIR, 'best_transformer_model.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in txt_state: txt_state = txt_state['state_dict']\n",
    "    text_model.load_state_dict(txt_state, strict=False) \n",
    "    text_model.eval()\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dl = DataLoader(EnsembleDataset(df, img_dir, tokenizer, transform), batch_size=32, shuffle=False)\n",
    "\n",
    "    img_preds, text_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            input_ids, masks = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            img_out = img_model(imgs)\n",
    "            # FIX: Use softmax and extend with correct shape (N, 5) instead of flattening\n",
    "            img_preds.extend(torch.softmax(img_out, dim=1).cpu().numpy())\n",
    "            \n",
    "            text_out = text_model(input_ids, masks)\n",
    "            text_probs.extend(torch.softmax(text_out, dim=1).cpu().numpy())\n",
    "\n",
    "    # 4. Concatenate Features: XGB(5) + Text(5) + Image(5) = 15 Features\n",
    "    # FIX: Removed reshape(-1, 1) and flatten() to align dimensions\n",
    "    return np.hstack([xgb_probs, np.array(text_probs), np.array(img_preds)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e11ef",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea0f2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Train...\n",
      "Generating features for 11994 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0c2450069943129e9053c088daa52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 375/375 [07:39<00:00,  1.23s/it]\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test...\n",
      "Generating features for 2999 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7cae4090e6467f900c9e2a0b7b0130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 94/94 [01:56<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP Meta-Learner...\n",
      "Epoch 90 Loss: 0.6593\n",
      "Meta-Model (MLP) Test Kappa: 0.4455\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(os.path.join(DATA_DIR, 'train/train.csv'))\n",
    "\n",
    "# Create Splits\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    full_df, full_df['AdoptionSpeed'], test_size=0.2, random_state=42  # Add stratify? , stratify=full_df['AdoptionSpeed']\n",
    ")\n",
    "\n",
    "# Generate Features (The \"Level 1\" Predictions)\n",
    "print(\"Generating Meta-Features for Train...\")\n",
    "X_train_meta = generate_ensemble_features(train_df) \n",
    "\n",
    "print(\"Generating Meta-Features for Test...\")\n",
    "X_test_meta = generate_ensemble_features(test_df)\n",
    "\n",
    "# - Use a Neural Network (Dense Layer) to combine & decide -\n",
    "class MetaModelMLP(nn.Module):\n",
    "    def __init__(self, input_dim=15, num_classes=5):\n",
    "        super(MetaModelMLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Prepare Data for PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train_meta).to(DEVICE)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(DEVICE)\n",
    "X_test_tensor = torch.FloatTensor(X_test_meta).to(DEVICE)\n",
    "\n",
    "# Training Loop\n",
    "meta_model = MetaModelMLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training MLP Meta-Learner...\")\n",
    "for epoch in range(100): # 100 epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = meta_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0: print(f\"Epoch {epoch} Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "# Evaluate\n",
    "meta_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = meta_model(X_test_tensor)\n",
    "    test_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"\\nMeta-Model (MLP) Test Kappa: {cohen_kappa_score(y_test, test_preds, weights='quadratic'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test Data...\n",
      "Generating features for 3972 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1331aa514554e34afb303a4836ed35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 125/125 [02:54<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to ../submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test.csv!\n",
    "# Load test data\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Generate meta-features for test data\n",
    "print(\"Generating Meta-Features for Test Data...\")\n",
    "X_test_final_meta = generate_ensemble_features(test_data, img_dir=IMAGE_DIR)\n",
    "\n",
    "# Convert to tensor\n",
    "X_test_final_tensor = torch.FloatTensor(X_test_final_meta).to(DEVICE)\n",
    "\n",
    "# Make predictions using trained meta model\n",
    "meta_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = meta_model(X_test_final_tensor)\n",
    "    final_test_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'PetID': test_data['PetID'],\n",
    "    'AdoptionSpeed': final_test_preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Submission saved to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
