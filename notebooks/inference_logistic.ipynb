{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For handling module in diff dir\n",
    "import sys\n",
    "import os \n",
    "\n",
    "# Config\n",
    "TEST_DATA_PATH = '../data/test/test.csv'\n",
    "IMAGE_DIR = '../data/test_images/'\n",
    "OUTPUT_PATH = '../submission.csv'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models/v1_stratify'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'train_images') \n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITIONS\n",
    "\n",
    "# 1. Text Model (Code from k4)\n",
    "class TransformerPetClassifier(nn.Module):\n",
    "    \"\"\"Transformer-based classifier for pet adoption speed prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=5, dropout=0.3):\n",
    "        super(TransformerPetClassifier, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Get hidden size from transformer config\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 2. Image Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 5)\n",
    "    def forward(self, x): return self.resnet(x)\n",
    "\n",
    "# 3. Tabular Model\n",
    "# no need to define xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85145e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tokenizer, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Image Processing\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['PetID']}-1.jpg\") \n",
    "        image = Image.new('RGB', (224, 224), (0, 0, 0)) \n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                pass \n",
    "        if self.transform: image = self.transform(image)\n",
    "\n",
    "        # 2. Text Processing\n",
    "        desc = str(row['Description']) if pd.notna(row['Description']) else \"no description\"\n",
    "        encoding = self.tokenizer(\n",
    "            desc, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe912386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_table(tabular_df):\n",
    "    # Namelength\n",
    "    tabular_df[\"name_length\"] = tabular_df['Name'].str.len().fillna(0)\n",
    "    \n",
    "    # Description length\n",
    "    tabular_df['description_length'] = tabular_df['Description'].str.len().fillna(0)\n",
    "    \n",
    "    \n",
    "    # Is Mixed Breed? (Breed2 is not 0)\n",
    "    tabular_df['is_mixed_breed'] = (tabular_df['Breed2'] != 0).astype(int)\n",
    "    \n",
    "    # Number of Colors (Count non-zero color columns)\n",
    "    tabular_df['num_colors'] = (tabular_df[['Color1', 'Color2', 'Color3']] != 0).sum(axis=1)\n",
    "    \n",
    "    # Is Free? (Fee is 0)\n",
    "    tabular_df['is_free'] = (tabular_df['Fee'] == 0).astype(int)\n",
    "\n",
    "    # Fee per Pet (Normizalized for litters)\n",
    "    tabular_df['fee_per_pet'] = tabular_df['Fee'] / tabular_df['Quantity'].replace(0, 1)\n",
    "\n",
    "    # Total Media (Engagement proxy)\n",
    "    tabular_df['total_media'] = tabular_df['PhotoAmt'] + tabular_df['VideoAmt']\n",
    "\n",
    "    # Health Issue Flag (Health > 1 implies injury or condition)\n",
    "    tabular_df['has_health_issue'] = (tabular_df['Health'] > 1).astype(int)\n",
    "    # --------------------\n",
    "    \n",
    "    # Encode state/breed as categories\n",
    "    # ADDED 'Type' to this list\n",
    "    cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', \n",
    "                    'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n",
    "                    'Sterilized', 'Health', 'State']\n",
    "    for col in cat_cols:\n",
    "        if col in tabular_df.columns:\n",
    "            tabular_df[col] = tabular_df[col].astype('category')\n",
    "    \"\"\"       \n",
    "    # Sentiment Analysis from JSON\n",
    "        print(\"Extracting sentiment features...\")\n",
    "        sentiment_features = tabular_df['PetID'].apply(lambda x: extract_sentiment(x, True))\n",
    "        tabular_df['sentiment_score'] = [x[0] for x in sentiment_features]\n",
    "        tabular_df['sentiment_magnitude'] = [x[1] for x in sentiment_features]\n",
    "    \"\"\"\n",
    "    return tabular_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff685371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_features(df, img_dir=IMG_DIR):\n",
    "    print(f\"Generating features for {len(df)} samples...\")\n",
    "    \n",
    "    # 1. XGBoost Inference\n",
    "    print(\"Loading XGBoost...\")\n",
    "    xgb_model = joblib.load(os.path.join(MODELS_DIR, 'xgb_stratify_optuna.pkl'))\n",
    "    \n",
    "    # Preprocess Tabular (Manual replication of src/tabular_model.py logic if not exposed as static method)\n",
    "    # Ideally: from src.tabular_model import TabularModel; TabularModel.preprocess(df)\n",
    "    \n",
    "    df_tab = featurize_table(df)\n",
    "    \n",
    "    drop_cols = ['Name', 'PetID', 'RescuerID', 'Description', 'AdoptionSpeed']\n",
    "    df_tab = df_tab.drop([c for c in drop_cols if c in df_tab.columns], axis=1)\n",
    "    \n",
    "    \n",
    "    xgb_probs = xgb_model.predict_proba(df_tab) # Shape (N, 5)\n",
    "\n",
    "    # 2. Load PyTorch Models\n",
    "    print(\"Loading DL Models...\")\n",
    "    # Image\n",
    "    img_model = ResNet().to(DEVICE)\n",
    "    img_state = torch.load(os.path.join(MODELS_DIR, 'pet_pred_resnet50.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in img_state: img_state = img_state['state_dict']\n",
    "    # FIX: Add 'resnet.' prefix to match the class definition\n",
    "    new_state_dict = {}\n",
    "    for k, v in img_state.items():\n",
    "        if not k.startswith('resnet.'):\n",
    "            new_state_dict['resnet.' + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    img_model.load_state_dict(new_state_dict)\n",
    "    img_model.eval()\n",
    "\n",
    "    # Text\n",
    "    text_model = TransformerPetClassifier(num_classes=5).to(DEVICE)\n",
    "    txt_state = torch.load(os.path.join(MODELS_DIR, 'best_transformer_model.pth'), map_location=DEVICE)\n",
    "    if 'state_dict' in txt_state: txt_state = txt_state['state_dict']\n",
    "    text_model.load_state_dict(txt_state, strict=False) \n",
    "    text_model.eval()\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dl = DataLoader(EnsembleDataset(df, img_dir, tokenizer, transform), batch_size=32, shuffle=False)\n",
    "\n",
    "    img_preds, text_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            input_ids, masks = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            img_out = img_model(imgs)\n",
    "            # FIX: Use softmax and extend with correct shape (N, 5) instead of flattening\n",
    "            img_preds.extend(torch.softmax(img_out, dim=1).cpu().numpy())\n",
    "            \n",
    "            text_out = text_model(input_ids, masks)\n",
    "            text_probs.extend(torch.softmax(text_out, dim=1).cpu().numpy())\n",
    "\n",
    "    # 4. Concatenate Features: XGB(5) + Text(5) + Image(5) = 15 Features\n",
    "    # FIX: Removed reshape(-1, 1) and flatten() to align dimensions\n",
    "    return np.hstack([xgb_probs, np.array(text_probs), np.array(img_preds)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e11ef",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Train...\n",
      "Generating features for 11994 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3654abb940444573a8870309294f031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 375/375 [07:24<00:00,  1.19s/it]\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test...\n",
      "Generating features for 2999 samples...\n",
      "Loading XGBoost...\n",
      "Loading DL Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134cb5f7d83a4987b820e02e5f316a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "100%|██████████| 94/94 [01:51<00:00,  1.19s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'oof_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m X_test_meta = generate_ensemble_features(test_df)\n\u001b[32m     25\u001b[39m meta_model = LogisticRegression()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m meta_model.fit(\u001b[43moof_preds\u001b[49m, full_df[\u001b[33m'\u001b[39m\u001b[33mAdoptionSpeed\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     30\u001b[39m meta_model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'oof_preds' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "full_df = pd.read_csv(os.path.join(DATA_DIR, 'train/train.csv'))\n",
    "\n",
    "# Create Splits\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    full_df, full_df['AdoptionSpeed'], test_size=0.2, random_state=42, stratify=full_df['AdoptionSpeed']  # Add stratify? , stratify=full_df['AdoptionSpeed']\n",
    ")\n",
    "# TODO: create a mapping for count encoding rescuerID -> for inference use the rescuer_counts to map \n",
    "rescuer_counts = train_df[\"RescuerID\"].value_counts()\n",
    "#rescuer_counts.to_csv('rescuer_counts.csv')\n",
    "train_df['rescuer_count'] = train_df['RescuerID'].map(rescuer_counts)\n",
    "\n",
    "test_df['rescuer_count'] = test_df['RescuerID'].map(rescuer_counts).fillna(0)\n",
    "\n",
    "train_df.drop('RescuerID', axis=1, inplace=True)\n",
    "test_df.drop('RescuerID', axis=1, inplace=True)\n",
    "# Generate Features (The \"Level 1\" Predictions)\n",
    "print(\"Generating Meta-Features for Train...\")\n",
    "X_train_meta = generate_ensemble_features(train_df) \n",
    "\n",
    "print(\"Generating Meta-Features for Test...\")\n",
    "X_test_meta = generate_ensemble_features(test_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ed4eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Meta-Model (Logistic) Test Kappa: 0.4040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X_train_meta, y_train)\n",
    "test_preds = meta_model.predict(X_test_meta)\n",
    "\n",
    "print(f\"\\nMeta-Model (Logistic) Test Kappa: {cohen_kappa_score(y_test, test_preds, weights='quadratic'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Meta-Features for Test Data...\n",
      "Generating features for 3972 samples...\n",
      "Loading XGBoost...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 28, got 27",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Generate meta-features for test data\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating Meta-Features for Test Data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_test_final_meta = \u001b[43mgenerate_ensemble_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMAGE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[32m     10\u001b[39m X_test_final_tensor = torch.FloatTensor(X_test_final_meta).to(DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mgenerate_ensemble_features\u001b[39m\u001b[34m(df, img_dir)\u001b[39m\n\u001b[32m     13\u001b[39m drop_cols = [\u001b[33m'\u001b[39m\u001b[33mName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPetID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRescuerID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAdoptionSpeed\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m df_tab = df_tab.drop([c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m drop_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_tab.columns], axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m xgb_probs = \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_tab\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Shape (N, 5)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 2. Load PyTorch Models\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading DL Models...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1923\u001b[39m, in \u001b[36mXGBClassifier.predict_proba\u001b[39m\u001b[34m(self, X, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1921\u001b[39m     class_prob = softmax(raw_predt, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   1922\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m class_prob\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m    \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _cls_predict_proba(\u001b[38;5;28mself\u001b[39m.n_classes_, class_probs, np.vstack)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1448\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1457\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Work\\DataScience\\AdoptMe\\.venv\\Lib\\site-packages\\xgboost\\core.py:2865\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2860\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2861\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`shape` attribute is required when `validate_features` is True\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2862\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2863\u001b[39m         )\n\u001b[32m   2864\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.shape) != \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_features() != data.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m2865\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2866\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2867\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2868\u001b[39m         )\n\u001b[32m   2870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[32m   2871\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[31mValueError\u001b[39m: Feature shape mismatch, expected: 28, got 27"
     ]
    }
   ],
   "source": [
    "# Prediction on test.csv!\n",
    "# Load test data\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "test_data = \n",
    "# Generate meta-features for test data\n",
    "print(\"Generating Meta-Features for Test Data...\")\n",
    "\n",
    "X_test_final_meta = generate_ensemble_features(test_data, img_dir=IMAGE_DIR)\n",
    "\n",
    "# Convert to tensor\n",
    "X_test_final_tensor = torch.FloatTensor(X_test_final_meta).to(DEVICE)\n",
    "\n",
    "# Make predictions using trained meta model\n",
    "meta_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = meta_model(X_test_final_tensor)\n",
    "    final_test_preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'PetID': test_data['PetID'],\n",
    "    'AdoptionSpeed': final_test_preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Submission saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a7920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m.save({\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m: model.state_dict(),\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m: MODEL_NAME,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m: NUM_CLASSES,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m: MAX_LENGTH,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_kappa\u001b[39m\u001b[33m'\u001b[39m: best_kappa\n\u001b[32m      7\u001b[39m }, \u001b[33m'\u001b[39m\u001b[33mmetamodel.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'best_kappa': best_kappa\n",
    "}, 'metamodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
